\section{Exploring Thread Interleaving Space}
\label{s:design}

In this section, we describe our approach to effectively achieve
\textbf{Design goal 1} and \textbf{2}.
%
We first explain the key idea of our approach~(\autoref{ss:overview}).
Then, we illustrate our concurrency coverage metric to express
combinatorial interleavings (\textbf{Design goal
  1})~(\autoref{ss:coverage}), and the interleaving search strategy to
cleverly explore thread interleavings (\textbf{Design goal
  2})~(\autoref{ss:scheduler}).
%While this section focuses only on how to explore the search space of
%thread interleaving with a given multi-thread input, the whole design
%of \sys will be explained later in \autoref{s:impl}.


\subsection{Key Idea: Segmentizing thread interleaving}
\label{ss:overview}

\dr{TODO: term consistency: interleaving order <-> interleaved instruction}

%\begin{table}[t]
  %\centering
  %input{table/learningfrommistakes.tex}
  %\caption{Statistics provided by Shan Lu
    %\etal~\cite{learningfrommistakes}, stating the number of
    %concurrency bugs according to the number of memory accesses
    %involved in the manifestation of a concurrency bug. \yj{Do we need
      %the table?} \dr{No. we can remove this.}}
  %\label{table:learningfrommistakes}
  % \end{table}

When fuzzing concurrency bugs with a coverage metric, the fundamental
challenge is the large search space because the combinations of
interleaving orders increase the search space exponentially.
%
Therefore, this work seeks to strike the balance in the trade-off
between the bug-finding capability and the search complexity.
%
Observed by a previous study~\cite{learningfrommistakes}, 92.4\% (97
out of 105) of concurrency bugs manifest when a bug-finding system
monitors interleaving orders of \textit{at most} four memory accesses
referring to shared memory objects.
%
All other memory accesses beyond four accesses and their execution
orders do not meaningfully affect manifestation of a concurrency bug.
%
The observation is also applied to the example of
\autoref{fig:cve-2017-17712}. The uninitialized access bug is
triggered by the execution order of three memory accesses (\eg,
\texttt{A2}, \texttt{A4} and \texttt{B1}), while others (\eg,
\texttt{A6} and \texttt{B2}) are irrelevant to manifest the
concurrency bug.


\PP{Segmentizing thread interleaving}
%
To reduce the search complexity, we take the classical wisdom of
problem decomposition where the complexity of a problem exponentially
decreases as the problem size decreases.
%
Our key strategy is \textit{decomposing} the search space into small
sub-spaces. We call a sub-space \textit{interleaving segment}.
%
Each interleaving segment consists of instructions accessing 
shared memory objects.
% the same  memory objects. (\ie, conflicting instructions).
%
%
When constructing an interleaving segment, following the observation, 
we confine the size of a interleaving segment 
to contain interleaved instructions including at most four memory accesses,
which makes the problem of defining a coverage metric tractable while maintaining a strong bug-finding capability.


\PP{Example of segmentizing thread interleaving}
%
\begin{figure}[t]
  \centering
  \includegraphics[width=0.8\linewidth]{fig/intuition.pdf}
  \caption{(a) A thread interleaving example of
    \autoref{fig:cve-2017-17712}, (b) instruction pairs that access
    the same memory object, and (c) instruction segments composed of
    two instruction pairs.  Note that we intentionally omit
    instructions that do not access globally-visible memory
    objects.}
  \label{fig:keyidea}
\end{figure}
%
\autoref{fig:keyidea} visualizes how we segmentize a thread
interleaving.
%
Let us assume we execute the two system calls in
\autoref{fig:cve-2017-17712} concurrently, and monitor memory accesses
annotated with timestamps.
%
We can then draw a totally-ordered instruction sequence of the thread
inteleaving as described in \autoref{fig:keyidea}-(a).
%

In this instruction sequence, we segmentize the thread interleaving in
two steps. First, we pair up two instructions that access the same
mermoy object.
%
As shown in \autoref{fig:keyidea}-(b), there are three pairs of
interleaved instructions, $\texttt{B1} \Rightarrow \texttt{A2}$,
$\texttt{B1} \Rightarrow \texttt{A4}$, and
$\texttt{A6} \Rightarrow \texttt{B2}$.
%
Since these pairs just describe a single interleaivng order, we
further combine them to represent a combination of interleaving orders
as the second step.
%
Since each pair contains two memory accesses, we select two pairs when
constructing interleaving segments to make them contain at most four
instructions.
%
For example, when we select two instruction pairs
$\texttt{B1} \Rightarrow \texttt{A2}$ and
$\texttt{B1} \Rightarrow \texttt{A4}$, there are three instructions
\texttt{B1}, \texttt{A2}, and \texttt{A4}.
%
As we assume that these memory accesses are annotated with timestmaps,
we can arrange them in \texttt{Segment \#1} accordingly.
%
Likewise, \texttt{Segment \#2} and \texttt{Segment \#3} are the result
of combining $\texttt{B1} \Rightarrow \texttt{A2}$ and
$\texttt{A6} \Rightarrow \texttt{B2}$, and
$\texttt{B1} \Rightarrow \texttt{A4}$ and
$\texttt{A6} \Rightarrow \texttt{B2}$ respectively.

%
% \autoref{fig:keyidea}-(a) represents the thread interleaving of the
% execution, where the uninitialized access bug does not manifest
% because \texttt{B1} is executed before \texttt{A4} (\ie,
% $(\texttt{A2} \Rightarrow \texttt{B1}) \wedge (\texttt{B1} \Rightarrow
% \texttt{A4})$ is not satisfied).
% %
% In order to track interleaving orders of a small number (\eg, four) of
% instructions, we decompose the thread interleaving into several
% interleaving segments as described in \autoref{fig:keyidea}-(b).
% %
% In these interleaving segments, \texttt{Segment \#1} contains three
% memory access operations (\ie, \texttt{A2}, \texttt{A4}, and
% \texttt{B1}), and describes interleaving orders such that \texttt{B1}
% is executed after \texttt{A2} and \texttt{A4}.
% %
% Similarly, \texttt{Segment \#2} and \texttt{Segment \#3} describes
% interleaving orders of four memory access operations, (\texttt{A2},
% \texttt{B1}, \texttt{B2}, \texttt{A6}) and (\texttt{A4}, \texttt{B1},
% \texttt{B2}, \texttt{A6}) respectively.
%

% With these interleaving segments, the fuzzer can be noticed that
% interleaving orders represented by these interelaving segments
% unlikley cause a concurrency bug.
% %
% Therefore, it is adequate for the fuzzer to search for unexplored
% interleaving segments (\eg, one that represents
% $(\texttt{A2} \Rightarrow \texttt{B1}) \wedge (\texttt{B1} \Rightarrow
% \texttt{A4})$) to maximize the chance of discovering concurrency bugs.


% %
% \dr{}
% It is worth noting that interleaving segments can be overlapped; in
% this example, \texttt{Segment \#1} and \texttt{Segment \#3} are
% overlapped over an interleaving order
% $\texttt{A4} \Rightarrow \texttt{B1}$.




\PP{Benefits of segmentizing thread interleaving}
%
Segmentizing thread interleaving provides two remarkable benefits to a
fuzzer as follows.



First, tracking interleaving segments is a befitting choice to
determine whether to further explore thread interleavings of a
multi-thread input.
%
% As mentioned above, most concurrency bugs manifest depending only on
% the execution order of four memory accesses.
%
Considering a concurrency bug manifests depending on a combination of
interleaving orders of at most four memory accesses (which is exactly
represented by an interleaving segment), if a fuzzer explores all
interleaving segments, it is unlikely that a fuzzer misses a
concurrency bugs.
%
Accordingly, \textit{interleaving segments can act as an
  interleaving coverage metric satisfying \textbf{Design goal 1}}.






Second, explored interleaving segments can be exploited to
systematically search thread interleavings to run in future
iterations.
%
Since each interleaving segment contains only a small number of memory
accesses, we can easily enumerate interleaving orders that these
memory accesses can produce.
%
Then, among enumerated interleaving orders, we can select ones that
unexplored before, and direct the fuzzer to search them.
%
In this way, \textit{we can satisfy \textbf{Design goal 2} by
  exploring meaningful thread interleavings at each iteration and
  minimizing redundant executions}.


% 
% \autoref{fig:hint} demonstrates how explored interleaving segments
% can be helpful for future iterations.
% %
% As illustrated in \autoref{fig:hint}, the fuzzer can derive
% \textit{unexplored} \texttt{Segment \#1*} by changing the execution
% order of \texttt{A4} and \texttt{B4} in \textit{explored}
% \texttt{Segment \#1}.
% %
% Since \texttt{Segment \#1*} satisfies
% $(\texttt{A2} \Rightarrow \texttt{B1}) \wedge (\texttt{B1} \Rightarrow
% \texttt{A4})$, the fuzzer can discover the uninitialized access bug if
% it executes thread interleaving containing \texttt{Segment \#1*}.
% %
% In addition, the fuzzer can explore multiple interleaving segments at
% a time.
% %
% Besides \texttt{Segment \#1*}, \texttt{Segment \#3*} can also be
% derived from \texttt{Segment \#3}.
% %
% Interestingly, \texttt{Segment \#1*} and \texttt{Segment \#3*} can be
% used to compose a new thread interleaving.
% %
% By executing a thread interleaving including the two derived
% interleaving segments, the fuzzer is able to quickly test a number of
% interleaving segments.
% %
% In this way, \textit{our proposed scheduler mechanism is designed to
%   quickly explore unexplored interleaving segments, and to satisfy
%   \textbf{R2}}.
% %
% \begin{figure}[t]
%   \centering
%   \includegraphics[width=0.9\linewidth]{fig/hint.pdf}
%   \caption{\texttt{Segment \#1} and \texttt{Segment \#3} are explored
%     interleaving segments in \autoref{fig:keyidea}.
%     %
%     From these two interleaving segments, our approach derives other
%     interleaving segments \texttt{Segment \#1*} and \texttt{\#3*}, and
%     schedules instructions to test the derived interleaving segments
%     at the same time.}
%   \label{fig:hint}
% \end{figure}
% %



\subsection{Interleaving Segment Coverage}
\label{ss:coverage}

\newcommand{\intcov}{interleaving segment coverage\xspace}
\newcommand{\Intcov}{Interleaving segment coverage\xspace}


Using a set of interleaving segments, we propose a novel concurrency coverage metric called \textit{\intcov}.
\Intcov is designed to track interleaving orders
within an interleaving segment and used for guiding a fuzzer to  
efficiently search possible instruction interleavings.
If \intcov of a given input is saturated, 
a fuzzer can confidently conclude that the input unlikely causes a concurrency bug and proceed to the next input.

\PP{Graph representation of interleaved instructions}
As discussed, \intcov contains multiple interleaved instructions, 
so \intcov cannot be presented as a single instruction interleaving 
(e.g., $I_x \rightarrow I_y$) used in the alias coverage used in KRace.
Instead, we represent each interleaving segment as a small directed acyclic
graph (DAG) called an interleaving segment graph (shortly, \textit{interleaving graph}), and \intcov is a collection of interleaving graphs.

In this graph, a vertex represents a memory-accessing instruction, 
and an edge represents an execution ordering between two memory-access instructions. 
Specifically, there are two types of edges, program-order edges and
interleaving-order edges.
%
A program-order edge indicates an execution order between two 
memory-accessing instruction in a single thread.
Whereas, an interleaving-order edge indicates the execution
order between two memory-access instructions that 1) access the same
shared data, 2) at least one of them is a write operation, and 3) are
executed by different threads.
\autoref{fig:interleavingsegmentgraph} illustrates how a interleaving 
graph is derived from a interleaving segment, \texttt{Segment \#1}.
This interleaving graph contains a program-order edge from \texttt{A2} to
\texttt{A4} representing ``\texttt{A2} is executed before \texttt{A4}
in thread~A''.
Similarly, the interleaving graph contains two interleaving-order
edges, expressing interleaving orders of \texttt{B1} $\Rightarrow$ \texttt{A2}
and \texttt{B1} $\Rightarrow$ \texttt{A4}.
%It contains an interleaving-order edge from \texttt{A2} to
%\texttt{B1}, because 1) they access \texttt{inet->hdrincl}, 2)
%\texttt{B1} is a write operations, and 3) they are executed by
%different threads. Likewise, the segment graph also contains an
%interleaving-order edge from \texttt{A4} to \texttt{B1}.



\begin{figure}[t]
  \centering
  \includegraphics[width=0.9\linewidth]{fig/interleavingsegmentgraph.pdf}
  \caption{(a) \texttt{Segment \#1} from \autoref{fig:keyidea}, and
    (b) its interleaving graph. In (b), a dotted arrow represents a
    program order, and solid arrows represent interleaving orders.}
  \label{fig:interleavingsegmentgraph}
\end{figure}

%It is worth noting that a segmentgraph has two properties. First, if
%a path exists from a vertex \texttt{X} to another vertex \texttt{Y}, a
%memory access operation corresponding to \texttt{X} is executed before
%a memory access operation corresponding to \texttt{Y}.
%%
%\dr{}
%This is because edges represent orderings that are a transitive
%relation.
%
%Second, \textit{all segment graph cannot contain a loop}.
%%
%If there is a loop exists, any vertex \texttt{Z} on the loop is
%executed before itself, which is contradictory.


%
% Let us represent a memory access operation $M$ as four tuples,
% $(tid, addr, op, timestamp)$ where $tid$ is the identity of a thread,
% $addr$ is the address of a memory location, $op$ is the type of the
% memory access operation (\ie, $store$ or $load$), and $timestamp$
% indicates the point of time when the memory access operation is taken.
% %
% $M(x)$ detnoes a field $x$ of $M$. For example, $M(tid)$ is a $tid$
% field of a memory access operation $M$.
% %
% Also let us suppose all memory access operations are totally
% ordered. \ie, there are no two memory access operations that have the
% same $timestamp$.

% For all pair of memory access operations $M_i$ and $M_j$, we define a
% scheduling constraint $SC$ as a tuple $(M_i, M_j)$ if
% $M_i(tid) \neq M_j(tid)$, $M_i(addr) = M_j(addr)$,
% $M_i(op) = store \vee M_j(op) = store$, and
% $M_i(timestamp) < M_j(timestamp)$.
% %
% Informally, $M_i$ and $M_j$ are conflicting memory acceess operations
% that are executed in other threads, and $M_i$ was taken place before
% $M_j$.
% %
% For two scheduling constraint $SC_1(M_{1i}, M_{1j})$ and
% $SC_2(M_{2i}, M_{2j})$, $SC_1 = SC_2$ if
% $(M_{1i} = M_{2i}) \wedge (M_{1j} = M_{2j})$.
% %
% Then, we define a scheduling constraint pair $SCPair = (SC_i, SC_j)$
% for two scheduling constraints if $i < j$, $SC_i \neq SC_j$.
% %
% Lastly, biconflict coverage of the concurrent job $BC\mbox{-}Cov$ is
% defined as a set of all scheduling constraint pairs,
% $\{SCPair_1, SCPair_2, ..., SCPair_n\}$, constructed from its memory
% access operation sequence.


\PP{Constructing interleaving segment coverage}
%
% While executing a multi-thread input, a fuzzer monitors
% memory-accessing instructions annotated with timestamps. After the
% execution is finished, a fuzzer constructs interelaving graphs to
% compute \intcov for the given input.
%
Here, we formally explain how to construct interleaving graphs. Let us
assume all memory-accessing instructions are uniquely labeled such as
$i_1$, $i_2$, ..., $i_k$.
%
A fuzzer generates vertices $v_1$, $v_2$, ..., $v_k$ for each
memory-accessing instructions, and also label them with their
corresponding instructions.
%
Then, for two vertices $v_x$ and $v_y$ that $i_x$ was executed before
$i_y$, a fuzzer generates directed edges $v_x$ $\rightarrow$ $v_y$ if
1) $v_x$ and $v_y$ were executed in the same thread (\ie, a
program-order edge), or 2) $v_x$ and $v_y$ accessed the same memory
object (\ie, an interleaving-order edge).
%
As a result, a fuzzer constructs a graph $g = (V, E)$ representing a
\textit{whole} thread interleaving.



Then, $g_i$ is a subgraph of $g$ that contains two interleaving-order
edges.
%
For all $e_z (v_a \rightarrow v_b), e_w (v_c \rightarrow v_d) \in E$
where both $e_z$ and $e_w$ are interleaving-order edges and
$e_z \neq e_w$, a fuzzer constructs an interleaving graph
$g_i = (V_i, E_i)$, where $V_i$ is a set of $\{v_a, v_b, v_c, v_d\}$,
and $E_i$ ia set of edges connecting vertices in $V_i$.
%
Lastly, a fuzzer gathers interelaving graphs in to a set of
interleaving graphs $G = \{g_1, g_2, ..., g_n\}$.






\PP{Tracking interleaving segment coverage}
%
In practice, \intcov contains a large number of interleaving graphs,
which consumes a large amount of memory even though the size of graphs
is small.
%
To reduce memory consumption, we hash each interleaving graph,
and interleaving segment coverage tracks a hash table of segment
graphs.



To hash an interleaving graph, we define a hash function $hash(g_i)$
for an interleaving graph $g_i = (V_i, E_i)$.
%
The key property of the $hash(g_i)$ is reflecting directions of edges,
so that a fuzzer can distinguish different interleavings of the same
vertices.
%
To this end, we adopt an idea of Merkel hashing~\cite{treehashing,
  treehashing2}.
%
In particular, for all vertices $v$ in $V_i$, we first calculate a
hash value of $v$, $hash(v)$, which reflects its out-going edges. With
$o_1, ..., o_m$ that $v \rightarrow o_x \in E_i$,
%

\[ hash(v) = \mathcal{H}(v.label {++} o_1.label {++} ... {++}
  o_m.label) \]

where $\mathcal{H}$ denotes the
non-cryptographic FNV hash function~\cite{fnv, fnv-go}, and ${++}$
denotes the label-concatenation operation.
%
Then,

\[
  hash(g) = \underset{v \in V_i}{\oplus} hash(v)
\]

where $\oplus$ denotes the XOR operation.

\dr{I want to explain why this hash function can be used to
  differentiate different interleavings with an example of
  \autoref{fig:interleavingmutation}, but we don't have much
  spaces. What about explaining this in appendix?}






\PP{Utilizing interleaving segment coverage}
%
\dr{I think it would be clearer not to mention alias coverage}
%
Compared to the alias coverage, \intcov and the form of interleaving graph
express semantically richer information. \Intcov captures combinations 
of multiple memory-accessing instructions as a unit of an interleaving segment, which allows a fuzzer to explore the large search space more quickly than using alias coverage, which uses a single instruction interleaving. Intuitively, across two different runs, \intcov can precisely tell 
their differences in terms of at least two instruction interleavings, 
but the alias coverage can tell at least one\yj{DR, it this claim OK to you?}.
\Intcov provides guiding feedback to a fuzzer.
\Intcov guides the fuzzer to identify that there are possibly more useful instruction interleavings remain untested. So, the fuzzer spends
more computing power to explore more interesting (unseen) instruction interleavings from \intcov. We emphasize that our searching strategy is 
\textit{not random} unlike previous approaches~\cite{krace, ski, pctalgorithm, muzz}.
%
We use\yj{transform? better word?} a newly found interleaving graph to
direct the fuzzer to systematically explore the search space of
instruction interleavings while minimizing redundant and useless
search trials. \autoref{ss:scheduler} discusses our search strategy.


\subsection{Coverage-directed Interleaving Search}
\label{ss:scheduler}
%
% One important role of segment graphs are to infer unexplored
% interleaving segments that the fuzzer will search for in future
% iterations.
% %
In this subsection, we explain how we utilize explored interleaving
segment graphs to systematically explor thread interleavings.

\PP{Interleaving segment mutation}
%
%
The first step is to infer \textit{unexplored} interleaving graphs
from \textit{explored} interleaving graphs.
%
Suppose $G = \{g_1, g_2, ..., g_n \}$ is given for a thread
interleaving.
%
Since $G$ is obtained after executing the thread interleaving, all
graphs in $G$ are \textit{explored}.
%
Then, for each interleaving segment graph $g_i \in G$, we drive other
interleaving segment graphs by permuting interleaving-order edges in
$g_i$.



\begin{figure}[t]
  \centering
  \includegraphics[width=0.9\linewidth]{fig/interleavingmutation.pdf}
  \caption{Example interleaving segment graphs derived from
    \texttt{Segment \#1}. The fuzzer will explore (a) and (b), but
    will not explore (c) contains a loop.}
  \label{fig:interleavingmutation}
\end{figure}
%
\autoref{fig:interleavingmutation} illustrates how we derive
interleaving graphs from $g_i$.
%
In this figure, $g_i$ is an interleaving graph in
\autoref{fig:interleavingsegmentgraph}-(b).
%
With this $g_i$, we \textit{flip} interleaving-order edges, resulting
in different interleaving graphs.
%
Here the semantic behind ``\textit{flipping an interleaving-order
  edge}'' is simple; it changes the interleaving order of an
instruction pair that access the shared data.
%
For example, flipping $\texttt{A4} \Rightarrow \texttt{B1}$ produces
an interleaving graph described in
\autoref{fig:interleavingmutation}-(a).
%
Likewise, we generate an interleaving graph of
\autoref{fig:interleavingmutation}-(b) by flipping both of
$\texttt{A2} \Rightarrow \texttt{B1}$ and
$\texttt{A4} \Rightarrow \texttt{B1}$.
%
%
Besides (a) and (b), however, flipping
$\texttt{A2} \Rightarrow \texttt{B1}$ generates an \textit{invalid}
segment graph described in \autoref{fig:interleavingmutation}-(c).
%
This is because this interleaving graph contains a loop; a loop
indicates a contradiction on the execution order that any vertex on
the loop should be executed before itself.







After deriving interleaving segment graphs for all $g_i$, we aggregate
all derived and unobserved interleaving segment graphs into
$G_{derived}$. and uses it to search for a thread interleaving to run.

% Therefore, in this fugure, we can derive two \textit{unobserved}
% interleaving segment graphs.






\PP{Composing a thread interleaving}
%
With $G_{derived}$, we could naively test unobserved interleaving
graphs in $G_{derived}$ one-by-one. But, it is inefficient. As
described in \dr{}, a number of interleaving segments can be explored
at once.
%
Thus, we choose a \textit{smarter} way to consume $G_{derived}$.
%
For each iteration, we select a subset of $G_{derived}$ called
$G^{*}_{derived}$, and search for a thread interleaving to explore all
unobserved interleaving segment graphs in $G^{*}_{derived}$ at once.

%
% As described  \autoref{fig:hint}, multiple segment graphs can be
% explored at a time (\ie, one thread interleaving comprises multiple
% interleaving segments).
% %
% Thus, our strategy to to quickly consume $S_{derived}$ (\ie, a set of
% derived and unexplored segment graphs) is selecting multiple
% interleaving segments from $S_{derived}$, and explore them altogether.

\dr{wip}



However, not all interleaving segments cannot be explored together.
%
For example, in \autoref{fig:hint}, (a) and (b) cannot be explored
together; the segment graph (a) requires \texttt{A2} to be executed
before \texttt{B1} while the segment graph (b) requires the opposite
execution order between \texttt{A2} and \texttt{B1}.
%
Therefore, the fuzzer repeatedly selects and tests a subset of
$S_{derived}$ that is contradiction-free (\ie, does not make a
contradiction on the execution order) until $S_{derived}$ is empty.


It may require a heavy computation to identify the largest
contradiction-free subset of $S_{derived}$.
%
Instead of finding an optimal solution, we choose to adopt a greedy
heuristic.
%
Specifically, we start with an empty set $\bar{S}$. Then, we iterate
over derived segment graphs contained in given $S_{derived}$, and
determine adding each segment graph into $\bar{S}$ forms a loop or
not. If it does not form a loop, we add the segment graph into
$\bar{S}$.
%
After iterating all derived segment graphs in $S_{derived}$, $\bar{S}$
does not contain a loop. The fuzzer then excludes selected segment
graphs from the set of all derived segment graphs (\ie,
$S_{derived} = S_{derived} \setminus \bar{S}$), and generates
scheduling points to explore $\bar{S}$.









\PP{Generating scheduling points}
%
Once $G^{*}_{derived}$ is given, scheduling points can be easily
generated by conducting a topological sort~\cite{topologicalsort}.
%
Since an imaginary interleaving graph is acyclic, a topological sort
always returns a sequence of vertices (\ie, instructions) that does
not violate a program order.
%
It is well known that the time complexity of a topological sort is
$O(V+E)$. Considering that the graph is sparse, $E$ is a small value
so the time complexity can be asymptotically considered as $O(V)$.
%
In this sequence, scheduling points are just instructions that the
preemption should happen; \ie, the next instruction is executed by a
different thread.




%%% Local Variables:
%%% mode: latex
%%% TeX-master: "p"
%%% End:
