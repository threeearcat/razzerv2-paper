\section{Exploring Thread Interleaving Space}
\label{s:design}

In this section, we describe our approach to effectively achieve
\textbf{Design goal 1} and \textbf{2}.
%
We first explain the key idea of our approach~(\autoref{ss:overview}).
Then, we illustrate our concurrency coverage metric to express
combinatorial interleavings (\textbf{Design goal
  1})~(\autoref{ss:coverage}), and the interleaving search strategy to
cleverly explore thread interleavings (\textbf{Design goal
  2})~(\autoref{ss:scheduler}).
%While this section focuses only on how to explore the search space of
%thread interleaving with a given multi-thread input, the whole design
%of \sys will be explained later in \autoref{s:impl}.


\subsection{Key Idea: Segmentizing thread interleaving}
\label{ss:overview}

\dr{term consistency: interleaving order <-> interleaved instruction}

%\begin{table}[t]
  %\centering
  %input{table/learningfrommistakes.tex}
  %\caption{Statistics provided by Shan Lu
    %\etal~\cite{learningfrommistakes}, stating the number of
    %concurrency bugs according to the number of memory accesses
    %involved in the manifestation of a concurrency bug. \yj{Do we need
      %the table?} \dr{No. we can remove this.}}
  %\label{table:learningfrommistakes}
  % \end{table}

When fuzzing concurrency bugs with a coverage metric, the fundamental
challenge is the large search space because the combinations of
interleaving orders increase the search space exponentially.
%
Therefore, this work seek to strike the balance in the trade-off
between the bug-finding capability and the search complexity.
%
Observed by a previous study~\cite{learningfrommistakes}, 92.4\% (97
out of 105) of concurrency bugs manifest when a bug-finding system
monitors interleaving orders of \textit{at most} four memory accesses
referring to shared memory objects.
%
All other memory accesses beyond four accesses and their execution
orders do not meaningfully affect manifestation of a concurrency bug.
%
The observation is also applied to the example of
\autoref{fig:cve-2017-17712}. The uninitialized access bug is
triggered by the execution order of three memory accesses (\eg,
\texttt{A2}, \texttt{A4} and \texttt{B1}), while others (\eg,
\texttt{A6} and \texttt{B2}) are irrelevant to manifest the
concurrency bug.


\PP{Segmentizing thread interleaving}
%
To reduce the search complexity, we take the classical wisdom of
problem decomposition where the complexity of a problem exponentially
decreases as the problem size decreases.
%
Our key strategy is \textit{decomposing} the search space into small
sub-spaces. We call a sub-space \textit{interleaving segment}.
%
Each interleaving segment consists of instructions accessing 
shared memory objects.
% the same  memory objects. (\ie, conflicting instructions).
%
%
When constructing an interleaving segment, following the observation, 
we confine the size of a interleaving segment 
to contain interleaved instructions including at most four memory accesses,
which makes the problem of defining a coverage metric tractable while maintaining a strong bug-finding capability.


\PP{Building segments}
%
\dr{TODO: rewrite}
%
In particular, once executing threads concurrently, we firstly model
the thread interleaving as a totally-ordered instruction sequence.
%
And then, we group a small number of instructions (\eg, four
instructions) and their interleaving orders as an interleaving segment.


\begin{figure*}[t]
  \centering
  \includegraphics[width=0.9\linewidth]{fig/intuition.pdf}
  \caption{(a) A thread interleaving example of
    \autoref{fig:cve-2017-17712} that does not cause the concurrency
    bug, (b) instruction pairs that access the same memory object, and
    (c) instruction segments composed of two instruction pairs.  Note
    that we intentionally omit instructions that do not access
    globally-visible memory objects.}
  \label{fig:keyidea}
\end{figure*}
%
\autoref{fig:keyidea} visualizes our key idea.
%
Let us assume we execute the two system calls in
\autoref{fig:cve-2017-17712} concurrently.
%
\autoref{fig:keyidea}-(a) represents the thread interleaving of the
execution, where the uninitialized access bug does not manifest
because \texttt{B1} is executed before \texttt{A4} (\ie,
$(\texttt{A2} \Rightarrow \texttt{B1}) \wedge (\texttt{B1} \Rightarrow
\texttt{A4})$ is not satisfied).
%
In order to track interleaving orders of a small number (\eg, four) of
instructions, we decompose the thread interleaving into several
interleaving segments as described in \autoref{fig:keyidea}-(b).
%
In these interleaving segments, \texttt{Segment \#1} contains three
memory access operations (\ie, \texttt{A2}, \texttt{A4}, and
\texttt{B1}), and describes interleaving orders such that \texttt{B1}
is executed after \texttt{A2} and \texttt{A4}.
%
Similarly, \texttt{Segment \#2} and \texttt{Segment \#3} describes
interleaving orders of four memory access operations, (\texttt{A2},
\texttt{B1}, \texttt{B2}, \texttt{A6}) and (\texttt{A4}, \texttt{B1},
\texttt{B2}, \texttt{A6}) respectively.
%

% With these interleaving segments, the fuzzer can be noticed that
% interleaving orders represented by these interelaving segments
% unlikley cause a concurrency bug.
% %
% Therefore, it is adequate for the fuzzer to search for unexplored
% interleaving segments (\eg, one that represents
% $(\texttt{A2} \Rightarrow \texttt{B1}) \wedge (\texttt{B1} \Rightarrow
% \texttt{A4})$) to maximize the chance of discovering concurrency bugs.


% %
% \dr{}
% It is worth noting that interleaving segments can be overlapped; in
% this example, \texttt{Segment \#1} and \texttt{Segment \#3} are
% overlapped over an interleaving order
% $\texttt{A4} \Rightarrow \texttt{B1}$.




\PP{Benefits of segmentizing thread interleaving}
%
Segmentizing thread interleaving provides two remarkable benefits to a
fuzzer as follows.



First, tracking interleaving segments is a befitting choice to
determine whether to further explore thread interleavings of a
multi-thread input.
%
% As mentioned above, most concurrency bugs manifest depending only on
% the execution order of four memory accesses.
%
Considering a concurrency bug manifests depending on a combination of
interleaving orders of at most four memory accesses (which is exactly
represented by an interleaving segment), if a fuzzer explores all
interleaving segments, it is unlikely that a fuzzer misses a
concurrency bugs.
%
Accordingly, \textit{interleaving segments can act as an
  interleaving coverage metric satisfying \textbf{Design goal 1}}.






Second, explored interleaving segments can be exploited to
infer unexplored interleaivng segments.
%
Since each interleaving segment contains only a small number of memory
accesses, we can easily enumerate interleaving orders that these
memory accesses can produce.
%
Then, among the interleaving oders, we can direct the fuzzer to search
such interleaving segments that are unexplored before.
%
In this way, we can search for thread interleavings in a systematic
and smart way, satisfying \textbf{Design goal 2}.


% 
% \autoref{fig:hint} demonstrates how explored interleaving segments
% can be helpful for future iterations.
% %
% As illustrated in \autoref{fig:hint}, the fuzzer can derive
% \textit{unexplored} \texttt{Segment \#1*} by changing the execution
% order of \texttt{A4} and \texttt{B4} in \textit{explored}
% \texttt{Segment \#1}.
% %
% Since \texttt{Segment \#1*} satisfies
% $(\texttt{A2} \Rightarrow \texttt{B1}) \wedge (\texttt{B1} \Rightarrow
% \texttt{A4})$, the fuzzer can discover the uninitialized access bug if
% it executes thread interleaving containing \texttt{Segment \#1*}.
% %
% In addition, the fuzzer can explore multiple interleaving segments at
% a time.
% %
% Besides \texttt{Segment \#1*}, \texttt{Segment \#3*} can also be
% derived from \texttt{Segment \#3}.
% %
% Interestingly, \texttt{Segment \#1*} and \texttt{Segment \#3*} can be
% used to compose a new thread interleaving.
% %
% By executing a thread interleaving including the two derived
% interleaving segments, the fuzzer is able to quickly test a number of
% interleaving segments.
% %
% In this way, \textit{our proposed scheduler mechanism is designed to
%   quickly explore unexplored interleaving segments, and to satisfy
%   \textbf{R2}}.
% %
% \begin{figure}[t]
%   \centering
%   \includegraphics[width=0.9\linewidth]{fig/hint.pdf}
%   \caption{\texttt{Segment \#1} and \texttt{Segment \#3} are explored
%     interleaving segments in \autoref{fig:keyidea}.
%     %
%     From these two interleaving segments, our approach derives other
%     interleaving segments \texttt{Segment \#1*} and \texttt{\#3*}, and
%     schedules instructions to test the derived interleaving segments
%     at the same time.}
%   \label{fig:hint}
% \end{figure}
% %



\subsection{Interleaving Segment Coverage}
\label{ss:coverage}

\newcommand{\intcov}{interleaving segment coverage\xspace}
\newcommand{\Intcov}{Interleaving segment coverage\xspace}


Using a set of interleaving segments, we propose a novel concurrency coverage metric called \textit{\intcov}.
\Intcov is designed to track interleaving orders
within an interleaving segment and used for guiding a fuzzer to  
efficiently search possible instruction interleavings.
If \Intcov of a given input is saturated, 
a fuzzer can confidently conclude that the input unlikely causes a concurrency bug and proceed to the next input.

\PP{Graph representation of interleaved instructions}
As discussed, \Intcov contains multiple interleaved instructions, 
so \intcov cannot be presented as a single instruction interleaving 
(e.g., $I_x \rightarrow I_y$) used in the alias coverage used in KRace.
Instead, we represent each interleaving segment as a small directed acyclic
graph (DAG) called an interleaving segment graph (shortly, \textit{interleaving graph}), and \intcov is a collection of interleaving graphs.
%
In this graph, a vertex represents a memory-accessing instruction, 
and an edge represents an execution ordering between two memory-access instructions. 
Specifically, there are two types of edges, program-order edges and
interleaving-order edges.
%
A program-order edge indicates an execution order between two 
memory-accessing instruction in a single thread.
Whereas, an interleaving-order edge indicates the execution
order between two memory-access instructions that 1) access the same
shared data, 2) at least one of them is a write operation, and 3) are
executed by different threads.
\autoref{fig:interleavingsegmentgraph} illustrates how a interleaving 
graph is derived from a interleaving segment, \texttt{Segment \#1}.
This interleaving graph contains a program-order edge from \texttt{A2} to
\texttt{A4} representing ``\texttt{A2} is executed before \texttt{A4}
in thread~A''.
Similarly, the interleaving graph contains two interleaving-order
edges, expressing execution orders of \texttt{A2} $\rightarrow$ \texttt{B1}
and \texttt{A4} $\rightarrow$ \texttt{B1}.
%It contains an interleaving-order edge from \texttt{A2} to
%\texttt{B1}, because 1) they access \texttt{inet->hdrincl}, 2)
%\texttt{B1} is a write operations, and 3) they are executed by
%different threads. Likewise, the segment graph also contains an
%interleaving-order edge from \texttt{A4} to \texttt{B1}.



\begin{figure}[t]
  \centering
  \includegraphics[width=0.9\linewidth]{fig/interleavingsegmentgraph.pdf}
  \caption{(a) \texttt{Segment \#1} from \autoref{fig:keyidea}, and
    (b) its interleaving graph. In (b) a dotted arrow represents a program
    order, and solid arrows represent interleaving orders.}
  \label{fig:interleavingsegmentgraph}
\end{figure}

%It is worth noting that a segmentgraph has two properties. First, if
%a path exists from a vertex \texttt{X} to another vertex \texttt{Y}, a
%memory access operation corresponding to \texttt{X} is executed before
%a memory access operation corresponding to \texttt{Y}.
%%
%\dr{}
%This is because edges represent orderings that are a transitive
%relation.
%
%Second, \textit{all segment graph cannot contain a loop}.
%%
%If there is a loop exists, any vertex \texttt{Z} on the loop is
%executed before itself, which is contradictory.


%
% Let us represent a memory access operation $M$ as four tuples,
% $(tid, addr, op, timestamp)$ where $tid$ is the identity of a thread,
% $addr$ is the address of a memory location, $op$ is the type of the
% memory access operation (\ie, $store$ or $load$), and $timestamp$
% indicates the point of time when the memory access operation is taken.
% %
% $M(x)$ detnoes a field $x$ of $M$. For example, $M(tid)$ is a $tid$
% field of a memory access operation $M$.
% %
% Also let us suppose all memory access operations are totally
% ordered. \ie, there are no two memory access operations that have the
% same $timestamp$.

% For all pair of memory access operations $M_i$ and $M_j$, we define a
% scheduling constraint $SC$ as a tuple $(M_i, M_j)$ if
% $M_i(tid) \neq M_j(tid)$, $M_i(addr) = M_j(addr)$,
% $M_i(op) = store \vee M_j(op) = store$, and
% $M_i(timestamp) < M_j(timestamp)$.
% %
% Informally, $M_i$ and $M_j$ are conflicting memory acceess operations
% that are executed in other threads, and $M_i$ was taken place before
% $M_j$.
% %
% For two scheduling constraint $SC_1(M_{1i}, M_{1j})$ and
% $SC_2(M_{2i}, M_{2j})$, $SC_1 = SC_2$ if
% $(M_{1i} = M_{2i}) \wedge (M_{1j} = M_{2j})$.
% %
% Then, we define a scheduling constraint pair $SCPair = (SC_i, SC_j)$
% for two scheduling constraints if $i < j$, $SC_i \neq SC_j$.
% %
% Lastly, biconflict coverage of the concurrent job $BC\mbox{-}Cov$ is
% defined as a set of all scheduling constraint pairs,
% $\{SCPair_1, SCPair_2, ..., SCPair_n\}$, constructed from its memory
% access operation sequence.


\PP{Tracking interleaving segment coverage}
%
We track interleaving segment coverage \textit{offline}. While
executing a multi-thread input, a fuzzer collects memory-access
instructions annotated with timestamps. After the execution is
finished, the fuzzer collect segments and then builds interleaving
graphs for each segment to compute \intcov for the given input.


In particular, we first identify interleaved instructions


In practice, \intcov contains a large number of interleaving graphs, 
which consumes a large amount of memory even though the size of graphs is 
small.
%
To reduce memory consumption, we hash each interleaving graph,
and interleaving segment coverage tracks a hash table of segment
graphs.
%
\dr{TODO: explain how to hash the graph}\yj{Fill this, I will edit later.}





\PP{Utilizing interleaving segment coverage}
%
\dr{I think it would be clearer not to mention alias coverage}
%
Compared to the alias coverage, \intcov and the form of interleaving graph
express semantically richer information. \Intcov captures combinations 
of multiple memory-accessing instructions as a unit of an interleaving segment, which allows a fuzzer to explore the large search space more quickly than using alias coverage, which uses a single instruction interleaving. Intuitively, across two different runs, \intcov can precisely tell 
their differences in terms of at least two instruction interleavings, 
but the alias coverage can tell at least one\yj{DR, it this claim OK to you?}.
\Intcov provides guiding feedback to a fuzzer.
\Intcov guides the fuzzer to identify that there are possibly more useful instruction interleavings remain untested. So, the fuzzer spends
more computing power to explore more interesting (unseen) instruction interleavings from \intcov. We emphasize that our searching strategy is 
\textit{not random} unlike previous approaches~\cite{krace, ski, pctalgorithm, muzz}.
%
We use\yj{transform? better word?} a newly found interleaving graph to
direct the fuzzer to systematically explore the search space of
instruction interleavings while minimizing redundant and useless
search trials. \autoref{ss:scheduler} discusses our search strategy.


\subsection{Coverage-directed Interleaving Search}
\label{ss:scheduler}
%
% One important role of segment graphs are to infer unexplored
% interleaving segments that the fuzzer will search for in future
% iterations.
% %
In this subsection, we explain how we utilize explored interleaving
segment graphs to search unexplored interleaving segment graphs.

\PP{Deriving unexplored interleaving segement}
%
The first step is to derive \textit{unexplored} interleaving segment
graphs from \textit{explored} interleaving segment graphs.

From the executing of a thread interleaving, we obtain a set of
multiple interleaving segment graphs, and let us denote
$G = \{g_1, g_2, ..., g_n \}$ as this set of interleaving graphs.
%
Since $G$ is obtained after executing the thread interleaving, all
graphs in $G$ are \textit{explored}.
%
Then, for each interleaving segment graph $g_i \in G$, we drive other
interleaving segment graphs by permuting interleaving-order edges in
$g_i$.



\begin{figure}[t]
  \centering
  \includegraphics[width=0.9\linewidth]{fig/interleavingmutation.pdf}
  \caption{Example interleaving segment graphs derived from
    \texttt{Segment \#1}. The fuzzer will explore (a) and (b), but
    will not explore (c) contains a loop.}
  \label{fig:interleavingmutation}
\end{figure}
%
\autoref{fig:interleavingmutation} illustrates how we derive
interleaving graphs from $g_i$.
%
In this figure, $g_i$ is an interleaving graph in
\autoref{fig:interleavingsegmentgraph}-(b).
%
With this $g_i$, we \textit{flip} interleaving-order edges, resulting
in different interleaving graphs.
%
Here the semantic behind ``\textit{flipping an interleaving-order
  edge}'' is simple; it changes the interleaving order of an
instruction pair that access the shared data.
%
For example, flipping $\texttt{A4} \Rightarrow \texttt{B1}$ produces
an interleaving graph described in
\autoref{fig:interleavingmutation}-(a).
%
Likewise, we generate an interleaving graph of
\autoref{fig:interleavingmutation}-(b) by flipping both of
$\texttt{A2} \Rightarrow \texttt{B1}$ and
$\texttt{A4} \Rightarrow \texttt{B1}$.
%
%
Besides (a) and (b), however, flipping
$\texttt{A2} \Rightarrow \texttt{B1}$ generates an \textit{invalid}
segment graph described in \autoref{fig:interleavingmutation}-(c).
%
This is because this interleaving graph contains a loop; a loop
indicates a contradiction on the execution order that any vertex on
the loop should be executed before itself.







After deriving interleaving segment graphs for all $g_i$, we aggregate
all derived and unobserved interleaving segment graphs into
$G_{derived}$. and uses it to search for a thread interleaving to run.

% Therefore, in this fugure, we can derive two \textit{unobserved}
% interleaving segment graphs.






\PP{Searching for a thread interleaving}
%
With $G_{derived}$, we could naively test unobserved interleaving
graphs in $G_{derived}$ one-by-one. But, it is inefficient. As
described in \dr{}, a number of interleaving segments can be explored
at once.
%
Thus, we choose a \textit{smarter} way to consume $G_{derived}$.
%
For each iteration, we select a subset of $G_{derived}$ called
$G^{*}_{derived}$, and search for a thread interleaving to explore all
unobserved interleaving segment graphs in $G^{*}_{derived}$ at once.

%
% As described  \autoref{fig:hint}, multiple segment graphs can be
% explored at a time (\ie, one thread interleaving comprises multiple
% interleaving segments).
% %
% Thus, our strategy to to quickly consume $S_{derived}$ (\ie, a set of
% derived and unexplored segment graphs) is selecting multiple
% interleaving segments from $S_{derived}$, and explore them altogether.

\dr{wip}



However, not all interleaving segments cannot be explored together.
%
For example, in \autoref{fig:hint}, (a) and (b) cannot be explored
together; the segment graph (a) requires \texttt{A2} to be executed
before \texttt{B1} while the segment graph (b) requires the opposite
execution order between \texttt{A2} and \texttt{B1}.
%
Therefore, the fuzzer repeatedly selects and tests a subset of
$S_{derived}$ that is contradiction-free (\ie, does not make a
contradiction on the execution order) until $S_{derived}$ is empty.


It may require a heavy computation to identify the largest
contradiction-free subset of $S_{derived}$.
%
Instead of finding an optimal solution, we choose to adopt a greedy
heuristic.
%
Specifically, we start with an empty set $\bar{S}$. Then, we iterate
over derived segment graphs contained in given $S_{derived}$, and
determine adding each segment graph into $\bar{S}$ forms a loop or
not. If it does not form a loop, we add the segment graph into
$\bar{S}$.
%
After iterating all derived segment graphs in $S_{derived}$, $\bar{S}$
does not contain a loop. The fuzzer then excludes selected segment
graphs from the set of all derived segment graphs (\ie,
$S_{derived} = S_{derived} \setminus \bar{S}$), and generates
scheduling points to explore $\bar{S}$.









\PP{Generating scheduling points}
%
Once $G^{*}_{derived}$ is given, scheduling points can be easily
generated by conducting a topological sort~\cite{topologicalsort}.
%
Since an imaginary interleaving graph is acyclic, a topological sort
always returns a sequence of vertices (\ie, instructions) that does
not violate a program order.
%
It is well known that the time complexity of a topological sort is
$O(V+E)$. Considering that the graph is sparse, $E$ is a small value
so the time complexity can be asymptotically considered as $O(V)$.
%
In this sequence, scheduling points are just instructions that the
preemption should happen; \ie, the next instruction is executed by a
different thread.




%%% Local Variables:
%%% mode: latex
%%% TeX-master: "p"
%%% End:
