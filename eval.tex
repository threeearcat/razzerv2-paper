\section{Evaluation}
\label{s:eval}

To verify the effectiveness of our approach, we evaluate \sys in
various aspects.
%
Specifically, we 1) demonstrate the practicality of \sys by providing
newly found concurrency bugs in the Linux
kernel~(\autoref{ss:realworldbugs}),
%
2) compare \sys against prior concurrency fuzzing
techniques~(\autoref{ss:comparison}), and
%
3) provide comprehensive performance characteristics of \sys (\eg,
throughput and overheads)~(\autoref{ss:characteristics}).
%

\subsection{Finding Real-world Concurrency Bugs}
\label{ss:realworldbugs}

In order to demonstrate the practicality of \sys, we run \sys to
discover concurrency bugs in the latest Linux kernel.

\begin{table*}[t]
  \centering
  \input{table/newbugs.tex}
  \caption{List of concurrency bugs newly discovered by \sys. The
    \texttt{Recurrent} column denotes that a crash was previously
    addressed but reoccurs even after its patch is applied.}
  \label{table:newbugs}
\end{table*}

\PP{Experimental setup.}
%
To discover new concurrency bugs, we evaluate \sys on a two-socket
machine equipped with Intel(R) Xeon(R) CPU E5-2683 v4 @ 2.10GHz (40M
cache) and 512\GB of RAM.
%
This machine provides 32 total cores and 64 total threads, and runs
Ubuntu Server 20.04.4 LTS on Linux 5.4.143 as a host operating system.
%
During our experiments, we launch 32 virtual machines (VMs) where each
VM is equipped with four vCPUs and 8\GB memory.

To build a guest kernel, we use a kernel configuration used by
\texttt{Syzkaller}~\cite{syzkaller} so that \texttt{Syzkaller} and
\sys search for bugs in similar kernel modules/subsystems.
%
We run intermittently \sys for approximately two months on latest
versions of the Linux kernel ranging from 5.19-rc2 to 6.0-rc7.


\PP{Newly found concurrency bugs.}
%
During our evaluation, \sys discovers 83 unique crash titles including
ones that \texttt{Syzkaller} also finds. Among them, \totalbugs are
newly identified as harmful concurrency bugs as summarized in
\autoref{table:newbugs}.
%
This table shows that \sys is able to find bugs across the entire
kernel from specific device drivers~(\eg, \texttt{\#1}, and
\texttt{\#6}) to various network subsystems (\eg, \texttt{\#2},
\texttt{\#16}, and \texttt{\#17}); Since \sys is not tailored to
specific subsystems, \sys entails the \textit{generality} and is
applicable to various subsystems.

\sys is also able to find not only less-harmful bugs such as warnings
(\eg, \texttt{\#12}) but also critical bugs such as memory corruptions
(\eg, \texttt{\#2}, \texttt{\#6}, and \texttt{\#14}).
%
It is worth noting that in our evaluation, unlike previous
works~\cite{snowboard, krace} that rely on data race
detectors~\cite{kcsan, tsan}, all concurrency bugs are found by
observable and harmful incidents such as kernel panics or KASAN
reports.
%
\dr{}
We believe data race detectors and our approach are complementary in
finding concurrency bugs. We discuss this in the discussion
section~(\autoref{s:discussion}).

Interestingly, we find that many of bugs was previously found and
addressed, but they reoccur possibly because of their incomplete
fixes~\cite{learningfrommistakes}.
%
In \autoref{table:newbugs}, three that are marked in the
\texttt{Recurrent} column are cases that concurrency bugs reoccur even
after their fixes are applied.
%
These cases emphasize the importance of effective testing even after
bugs are exposed and fixed accordingly.





\subsection{Comparison with prior approaches}
\label{ss:comparison}

\begin{table}[t]
  \input{table/knownbugs.tex}
  \centering
  \caption{Known concurrency bugs that are studied in previous works,
    MoonShine~\cite{moonshine}, Razzer~\cite{razzer},
    ExpRace~\cite{exprace}, FUZE~\cite{fuze}, and
    Snowboard~\cite{snowboard}.}
  \label{table:knownbugs}
\end{table}

We compare \sys against various prior approaches 1) to demonstrate its
performance improvement in discovering concurrency bugs
(\autoref{sss:interleavingsearch}), and 2) justify the design choice
of interleaivng segment coverage (\autoref{sss:interleavingcoverage}).

% interleaving segment coverage with alias coverage~\cite{krace} and
% concurrent call
% pair~\cite{conzzer}~(\autoref{sss:interleavingcoverage}).


\PP{Bug selection}
%
\autoref{table:knownbugs} represents concurrency bugs we use for the
comparison study.
%
For a fair comparison, our concurrency bug selection criteria are 1)
concurrency bugs are used in previous studies on previous
studies~\cite{exprace, razzer, snowboard, moonshine, fuze}, and 2)
their exploits are publicly available so that we can make use of them
for our evaluation.

But there are few exceptions as follows: Although the exploit of
\texttt{69e16d01d1de}~\cite{snowboardbug} is not publicly available,
we successfully reproduce the concurrency bug from the description of
the Snowboard~\cite{snowboard} paper. Thus, we inlcude this
concurrency bug into the concurrency bug list for evaluation.
%
Whereas, even though Krace~\cite{krace} studies kernel concurrency
bugs (\ie, data races), we do not take concurrency bugs from Krace,
because we do not have access to ones used in Krace.
%
Also, we exclude two Android-related concurrency bugs (\ie,
CVE-2019-1999~\cite{cve20191999} and CVE-2019-2025~\cite{cve20192025})
evaluted by ExpRace~\cite{exprace} because their exploits require to
execute Android-specific code spaces where the current implementation
of \sys (and \texttt{Syzkaller}) is limited to exploring.



\PP{Kernel preparation}
%
Since concurrency bugs in \autoref{table:knownbugs} are introduced,
found, and fixed at different times, it is hard to find a kernel
version that is vulnerable to all listed concurrency bugs.
%
Therefore, we inject the concurrency bugs into the Linux kernel
version v6.0-rc7 by reverting patches fixing the concurrency bugs.






\subsubsection{Comparison of interleaving search strategies}
\label{sss:interleavingsearch}
%
% \begin{figure*}[t]
%   \centering
%   \includegraphics[width=\linewidth]{fig/comparison_graph-crop.pdf}
%   \caption{\dr{table is better?}}
%   \label{fig:eval:comparison}
% \end{figure*}
%
\begin{table*}
  \centering
  \input{table/comparison-interleaving-search.tex}
  \caption{Result of the comparison study against various
    state-of-the-art kernel concurrency fuzzers (\ie,
    Krace~\cite{krace} and Snowboard~\cite{snowboard}). We measure the
    number of executions and the elapsed time (secs) required to
    discover each concurrency bug. The \texttt{Naive} column indicates
    the kernel's scheduler (\ie, no thread scheduling control applied).}
  \label{table:comparison-interleaving-search}
\end{table*}
%
We measure the elapsed time and number of executions required to
discover a given concurrency bug to demonstrate the performance
improvements of \sys against recent works.

\PP{Comparison target}
%
We compare \sys to state-of-the-art kernel concurrency fuzzers,
Snowboard~\cite{snowboard}, and Krace~\cite{krace}.
%
Unfortunately, we cannot directly run Snowboard and Krace for the
comparison study becaues Krace is implemented only for file systems,
and Snowboard is \dr{}.
%
Therefore, we implement their approaches on the multi-thread fuzzing
phase~(\autoref{sss:multithreadfuzzing}) of \sys by applying 1) the
random delay injection scheme of Krace, and 2) the
enforcing-single-interleaving-order scheme of Snowboard.
%
In addition of recent concurrency fuzzing works, we also compare with
the kernel scheduler (\ie, no thread scheduling control applied) as
the baseline.


\PP{Comparison method}
%
With these kernel concurrency fuzzers, we measure the number of
executions and the elapsed time required to discover concurrency bugs.
%
However, these metrics heavily depend on the randomness and initial
seeds, which are known as impediments for a fair fuzzing
evaluation~\cite{fuzzingeval}.
%
In order to eliminate impacts of such impediments, we disable the
single-thread fuzzing~(\autoref{sss:singlethreadfuzzing}), and
manually provide a multi-thread input (including a pair of system
calls) that can cause each concurrency bug.
%
In this way, we eliminate the impact of how a fuzzer generates an
input which is highly affectted by the randomness, and can focus on
the effectiveness of the thread interleaving exploration of each
fuzzer.
%
Then, we observe how each fuzzer explores thread interleavings of a
given pair of system calls. During the evaluation, we limit the number
of executions to 10000.



% %
% Arguably, the most straightforward metric to compare fuzzing
% techniques is the elapsed time until concurrency bugs are discovered.
% %
% However, the elapsed time heavily depends on the randomness;
% \dr{because a fuzzer generates an input program very randomly, it is
%   possible that one fuzzer quickly generates an input program that
%   causes a concurrency bug, while another fuzzer takes very long time
%   to generate the input program}.

% In order to minimize the impact of the randomness and to concentrate
% on the performance impact of scheduling mechanisms, we predefine a
% multi-thread input as shown in \autoref{fig:multithreadinput}, and let
% a fuzzer repeatedly execute the given multi-thread input without
% generating new inputs nor mutating syscalls in the given input.

% In addition, previous fuzzing approaches

\PP{Result}
%
\autoref{table:comparison-interleaving-search} shows the comparison
result.
%
In this table, \texttt{\sys}, \texttt{Snowboard}, and \texttt{Krace}
columns display the number of executions and the elapsed time taken by
their corresponding works, and the \texttt{Naive} column corresponds
to the kernel scheduler without any thread scheduling control enabled.
%

%
Looking at the \texttt{Naive} column, we can identify the difficulty
of discovering varies from bug to bug.
%
For example, CVE-2018-12232 appears as the easiest concurrency bug to
discover since it can be discovered within 40 executions even with the
kernel scheduler.
%
On the other hand, the kernel scheduler fails to discover three
concurrency bugs, CVE-2019-6974, CVE-2019-11486, and
\texttt{69e16d01d1de} within 10K times of executions.


Regardless of the difficulty of discovering concurrency bugs, however,
\sys can discover all of concurrency bugs very effectively.
%
\sys can discover given concurrency bugs in just 9.3 runs on average,
ranging from 3 to 21.
%
\dr{}Whereas, Snowboard discovers concurrency bugs in XXX runs on
average, ragning from YYY to ZZZ, and Krace discovers given
concurrency bugs in KKK runs on average, raning from AAA to
BBB. Moreover, Krace even fails to discover CVE-2019-6974 in our
experiment.
%
As a consequence, we confirm that our approaches and \sys is effective
in discovering concurrency bugs.




\subsubsection{Justification of interleaving segment coverage}
\label{sss:interleavingcoverage}
%
We also justify our design choice of interleaving segment coverage by
showing what happens if a fuzzer adopts less-informative interleaving
coverage.


\PP{Comparison methodology}
%


\PP{Result}
%
\begin{table}[t]
  \small
  \centering
  \input{table/aliascoverage}
  \caption{\dr{aliascoverage}}
  \label{table:aliascoverage}
\end{table}


As expected, 










\subsection{Performance characteristics of \sys}
\label{ss:characteristics}

In this subsection, we analyze various performance characteristics of
\sys to comprehend how our approach affects the fuzzing process.
%
\subsubsection{All-inclusive evaluation}
\label{sss:allinclusive}

We first provide performance characteristics of the whole \sys.


\PP{Coverage growth.}
%
\begin{figure}[t]
  \centering
  \includegraphics[width=0.9\linewidth]{fig/coverage_graph-crop.pdf}
  \caption{Coverage growth of \sys.}
  \label{fig:eval:coverage}
\end{figure}
%
Since coverage metrics are the paramount performance metric of
fuzzing, we measure the coverage growth for both code coverage (\ie,
the number of taken branches) and interelaving coverage (\ie, the
number of observed interleaving segments) during 100 hours of fuzzing.

As shown in \autoref{fig:eval:coverage}, there is a notable difference
in scale between code coverage (a line denoted by \texttt{Branch}) and
interleaving coverage (a line denoted by \texttt{Interleaving segment}).
%
While the number of taken branches just reaches to 300K, the number of
interleaving segments is over 20M. Thus, the scale of interleaving
coverage is more than 66 times of code coverage during our evaluation.
%
This result follows the conventional belief that the search space of
thread interleaving is very large.

One thing to note about this scale difference is that storing
interleaving segment coverage consumes more memory than storing branch
coverage.
%
In our implementation, both branch coverage and interleaving segment
coverage are represented as hash tables where each element is
8-byte. Therefore, storing interleaving segment coverage consumes
about 180\MB while storing branch coverage requires 2\MB.
%
We can consider this result as a traditional space-time
tradeoff~\cite{spacetimetradeoff}; we invest \textit{more memory} to
discover concurrency bugs \textit{faster}.
%
% In addition, considering each VM is equipped with 8\GB memory, it is
% endurable for the fuzzer to store interleaving segment coverage using
% 180\MB (or even using ten times of 180\MB).


\PP{Fuzzing throughput.}
%
\begin{table}[t]
  \small
  \centering
  \input{table/throughput}
  \caption{Fuzzing throughput (\# of exec/s) of \sys and
    \texttt{Syzkaller}. \texttt{Syzkaller-memtrace} indicates
    throughput of \texttt{Syzkaller} with memory access tracing
    enabled.}
  \label{table:throughput}
\end{table}
%
All \sys's mechanisms provide benefits in finding concurrency bugs
with a cost of additional overheads and throughput degradation.
%
To comprehend the trade-off, we measure the fuzzing throughput of \sys
and compare it with the \texttt{Syzkaller}'s throughput.
%
In order to experiment in the same environment, we measure throughput
with an empty set of seed. And because both \texttt{Syzkaller} and
\sys restart VMs after an hour of fuzzing, we measure throughput in an
hour of execution in order to eliminate noises caused by, for example,
VM rebooting or kernel crashes.



\autoref{table:throughput} shows the result. As expected, \sys shows
the lower throughput than \texttt{Syzkaller}. In particular, the
\sys's throughput is about 54\% of the \texttt{Syzkaller}'s
throughput.
%
To further understand why the \sys's throughput is degraded, we
additionally measure throughput of \texttt{Syzkaller} while tracing
memory accesses (through instrumentation described in
\autoref{ss:instrumentation}), but not making use of it.
%
As shown in the \texttt{Syzkaller-memtrace} column in
\autoref{table:throughput}, it shows the throughput similar to that of
\sys; the \texttt{Syzkaller-memtrace}'s throughput is just 4.1\%
higher than the throughput of \sys.


These results indicate that the throughput of \sys is mainly degraded
by the heavy instrumentation to trace memory accesses.
%
However, as Krace~\cite{krace} states, it can be understandable as a
trade-off between throughput and input quality.
%
In the fuzzer's perspective, while tracking memory accesses has
negative impacts on throughput, it provides a higher chance for a
fuzzer to execute more interesting inputs (\ie, interesting thread
interleaving) and not to waste computing resources.
%
The effectiveness of high input quality is pronounced in
\autoref{ss:comparison}, showing \sys can discover concurrency bugs
extremely fast.




\PP{Per-input overhead}
%
\begin{table}[t]
  \centering
  \input{table/elapsedtime.tex}
  \caption{
    %
    Elapsed time (ms) for executing one multi-thread input. We
    decompose the elapsed time into the system call execution
    (\texttt{Exec. syscall}), \sys's computational overheads
    (\texttt{Comp. overhead}) and runtime overhead (\texttt{Runtime
      overhead}).}
  \label{table:elapsedtime}
\end{table}
%
In \sys, there are two types of overheads for executing a single
multi-thread input, such as computational overheads, and runtime
overheads.
%
Specifically, computation overheads are caused by tracking
interleaving segment coverage after executing the
input~(\autoref{ss:coverage}), and calculating scheduling points
before executing the input~(\autoref{ss:scheduler}).
%
On the other hand, runtime overheads are caused by tracing memory
accesses~(\autoref{ss:instrumentation}) and controlling thread
scheduling~(\autoref{ss:engine}).


To closely examine these overheads, we measure the elapsed time for
executing a single multi-thread input, and break down the elapsed time
into time taken by each components (\ie, tracking interleaving segment
coverage~(\autoref{ss:coverage}), searching a thrad
interleaving~(\autoref{ss:scheduler}), tracing memory
accesses~(\autoref{ss:instrumentation}), and controlling thread
scheduling~(\autoref{ss:engine})).
%
For this measurement, we run 10 thousands times and take an average.




\autoref{table:elapsedtime} shows the result. When executing a single
input, the total elapsed time is 267.2ms.
%
During the executioon, the part that took the longest time is
executing system calls; it takes 107.6ms.
%
However, overheads incurred by \sys is not negligible. Tracing memory
accesses~(\autoref{ss:instrumentation}) takes 90.7ms, and controlling
thread scheduling~(\autoref{ss:engine}) takes 42.8ms. These two
runtime overheads almost double the execution time, and are the main
cause of degrading the throughput of fuzzing as shown in the above.
%
In contrast, the total amound of time for computation is 26.1 (= 8.9 +
17.2)ms, and occupies approximately 9\% of the total elapsed time.
%
Accordingly, we can see that the computational overhead is relatively
small.



\subsubsection{Impact on coverage growth}
\label{sss:component}
%
Here, we present the impact of our design choices on both the
interleaving coverage growth and the code coverage growth.


\PP{Impact on interleaving exploration}
%
As the primiary purpose of this work is to effectively explore thread
interleavings, the \sys's approach~(\autoref{s:design}) should
contribute in effectively expanding the interleaving coverage.
%
To see how much \sys improves thread interleaving exploration, we
disable the thread scheduling control in the multi-thread fuzzing
phase of \sys, and measure the interleaving coverage.
%
The result is described in a line denoted by \texttt{Interleaivng
  segment w/o scheduling control} in \autoref{fig:eval:coverage}.
%
With the thread scheduling control disabled, \sys finds 29.1\% less
interleaving segment coverage during the same period.
%
As a consequence, we can concolude that our design choices
significantly improve in exploring thread interleavings.
%
% As a consequence, \sys can effectively discover concurrency bugs as
% shown in \autoref{ss:comparison}.



\PP{Impact on code exploration}
%
Since \sys invests the computing power to repeatedly execute a
multi-thread input (\ie, the multi-thread fuzzing phase), we expect
that \sys might explore code coverage less than the baseline
\texttt{Syzkaller}.
%
To see the difference of the code coverage exploration in \sys and
\texttt{Syzkaller}, we measure code coverage of \texttt{Syzkaller} and
illustrate it as a line denoted by \texttt{Branch (Syzkaller)} in
\autoref{fig:eval:coverage}.
%
As a result, \sys finds 3.2\% less code coverage compared to the
baseline \texttt{Syzkaller}.
%
While this is a definitely downside of \sys. However, considering the
huge benefit in exploring thread interleavings, we still believe that
this is marginal.





%%% Local Variables:
%%% mode: latex
%%% TeX-master: "p"
%%% End:
