\section{Evaluation}
\label{s:eval}

To demonstrate the effectiveness of our approach, 
% we evaluate \sys in various aspects.
% %
% Specifically,
%
we 1) demonstrate the usefulness of \sys by providing
newly found concurrency bugs in the Linux
kernel~(\autoref{ss:realworldbugs}),
%
2) quantitatively compare \sys against prior concurrency fuzzing
techniques~(\autoref{ss:comparison}), and
%
3) analyze comprehensive performance characteristics of
\sys~(\autoref{ss:characteristics}).
%

\subsection{Finding Real-world Concurrency Bugs}
\label{ss:realworldbugs}
%We run \sys to discover concurrency bugs in the latest Linux kernel.
\begin{table*}[t]
  \centering
  \input{table/newbugs.tex}
  \caption{List of concurrency bugs newly discovered by \sys. The
    \texttt{Recurrent} column denotes that a crash was previously
    addressed but reoccurs even after its patch is applied.}
  \label{table:newbugs}
  \vspace{-8pt}
\end{table*}

\PP{Experimental setup.}
%
We run \sys on a two-socket machine equipped with
Intel(R) Xeon(R) CPU E5-2683 v4 @ 2.10GHz (40M cache) and 512\GB of
RAM.
%
This machine provides 32 total cores and 64 total threads, and runs
Ubuntu Server 20.04.4 LTS on Linux 5.4.143 as a host operating system.
%
During our experiments, we launch 32 virtual machines (VMs) where each
VM is equipped with four vCPUs and 8\GB memory.
%
We use a Linux kernel configuration used by
\texttt{Syzkaller}~\cite{syzkaller} so that \texttt{Syzkaller} and
\sys search the same kernel modules/subsystems.
The kernel versions we run on \sys ranges from 5.19-rc2 to 6.0-rc7.
%
%We run intermittently \sys for approximately two months on latest
%versions of the Linux kernel ranging from 5.19-rc2 to 6.0-rc7.


\PP{Newly found concurrency bugs.}
%
During our evaluation period, \sys discovers 83 unique crash titles including
ones that \texttt{Syzkaller} also finds. Among them, \totalbugs are
newly identified as harmful concurrency bugs. The result is summarized in
\autoref{table:newbugs}.
%
This table shows that \sys is able to find bugs across the entire
kernel layers from specific device drivers~(\eg, \texttt{\#1}, and
\texttt{\#6}) to various network subsystems (\eg, \texttt{\#2},
\texttt{\#16}, and \texttt{\#17}). 
Unlike KRACE, which focuses on 
kernel file systems, \sys is not tailored to specific subsystems.
%\sys entails the \textit{generality} and is
%applicable to various subsystems.
%
\sys is able to find not only less-harmful bugs such as warnings
(\eg, \texttt{\#12}) but also critical bugs such as memory corruptions
(\eg, \texttt{\#2}, \texttt{\#6}, and \texttt{\#14}).
%
It is worth noting that in our evaluation, all concurrency bugs are
found by observable and harmful incidents such as kernel panics or
KASAN reports.
%
Interestingly, we find that some bugs were previously found and
patched, but they reoccur in later kernel version because the patches are
incomplete fixes~\cite{learningfrommistakes}.
%
In \autoref{table:newbugs}, three that are marked in the
\texttt{Recurrent} column shows such cases.
%
These cases underline the significance of thorough testing even after 
bugs have been identified and supposedly fixed.

\subsection{Comparison with prior approaches}
\label{ss:comparison}

\begin{table}[t]
  \input{table/knownbugs.tex}
  \centering
  \caption{Known concurrency bugs that are studied in previous works,
    MoonShine~\cite{moonshine}, Razzer~\cite{razzer},
    ExpRace~\cite{exprace}, FUZE~\cite{fuze}, and
    Snowboard~\cite{snowboard}.}
  \label{table:knownbugs}
  \vspace{-8pt}
\end{table}

We compare \sys against prior approaches to answer the following
questions: 1) is interleaving segment
coverage~(\autoref{ss:coverage}) informative and helpful in
discovering concurrency bugs?  (\textbf{Design goal 1} in
\autoref{s:motivation})~(\autoref{sss:interleavingcoverage}), and
%
2) how is the mutation-based interleaving
exploration~(\autoref{ss:scheduler}) effective in exploring the search
space of thread interleavings? (\textbf{Design goal
  2})~(\autoref{sss:interleavingsearch}).
%
To evaluate the points, we collect well-known concurrency bugs as a test set, 
and then run \sys and prior approaches on the test set.

% 1) to demonstrate its
% performance improvement in discovering concurrency bugs
% (\autoref{sss:interleavingsearch}), and 2) justify the design choice
% of interleaivng segment coverage (\autoref{sss:interleavingcoverage}).

% interleaving segment coverage with alias coverage~\cite{krace} and
% concurrent call
% pair~\cite{conzzer}~(\autoref{sss:interleavingcoverage}).



\PP{Bug selection}
%
\autoref{table:knownbugs} shows concurrency bugs for the
comparison study.
%
Our concurrency bug selection criteria are 1)
concurrency bugs are used in previous studies~\cite{exprace, razzer,
  snowboard, moonshine, fuze}, and 2) their exploits are publicly
available so that we can make use of them for our evaluation.
%
However, there are few exceptions as follows: Although the exploit of
\texttt{69e16d01d1de}~\cite{snowboardbug} is not publicly available,
we successfully reproduce the concurrency bug from the description of
the Snowboard~\cite{snowboard} paper, so we include the bug.
%
% Whereas, even though Krace~\cite{krace} studies a subset of kernel
% concurrency bugs (\ie, data races), we do not take concurrency bugs
% from Krace, because we do not have access to ones used in Krace.
%
We exclude two Android-related concurrency bugs (\ie,
CVE-2019-1999~\cite{cve20191999} and CVE-2019-2025~\cite{cve20192025})
evaluated in ExpRace~\cite{exprace}.
%, because their exploits require to
%execute Android-specific code spaces where the current implementation
%of \sys (and \texttt{Syzkaller}) is limited to exploring.
To run tests on a consistent environment, we use the same kernel 
of version v6.0-rc7 and make the bugs available by rolling back patches 
that addressed concurrency bugs.

%\PP{Kernel preparation}
%
%Since concurrency bugs in \autoref{table:knownbugs} are introduced,
%found, and fixed at different times, it is hard to find a kernel
%version that is vulnerable to all listed bugs.
%
%Therefore, we inject the concurrency bugs into the Linux kernel
%version v6.0-rc7 by reverting patches fixing the concurrency bugs.



\PP{Comparison method}
%
We measure the number of executions and the elapsed time required to
discover each concurrency bug.
%
However, these metrics heavily depend on initial seeds and the process
of mutating multi-thread input seeds, which may vary across evaluations and 
disturb a fair fuzzing evaluation~\cite{fuzzingeval}.
%
To confine such noisy interference, we manually provide
a multi-thread input as well as a pair of system calls that triggers
each concurrency bug.
Therefore, we evenly compare how accurately and quickly fuzzers can discover 
concurrency bugs when bug-triggering inputs are given.
%
%Then, in all evaluations in this section, a fuzzer only explores
%thread interleavings of the multi-thread inputs we provide, without
%generating or mutating inputs.
%
%In this way, we do not need to consider the process of mutation, and
%can focus on the effectiveness of the thread interleaving exploration
%of each approach.
%
Also, during the evaluation, we limit the number of executions to
10000 for each measurement, which is large enough for our evaluation.
%
% \dr{I do not use the term ``random'' here because the thread
%   interleaving exploration is random anyway.}





\subsubsection{Comparison of interleaving coverage metrics}
\label{sss:interleavingcoverage}
%
In order to answer the question why a concurrency fuzzer should adopt an
informative interleaving coverage metric (\ie, interleaving segment
coverage), we demonstrate performance of \sys if it adopts a
less-informative interleaving coverage metric (\eg, alias
coverage~\cite{krace}).


\PP{Comparison target}
%
We choose alias coverage as a comparison target since it is an
interleaving coverage metric most relevant to interleaving segment
coverage (\ie, both describe interleavings of instructions).
%
However, alias coverage is implemented only for file systems, so we
emulate alias coverage in \sys by limiting the number of vertices of
each segment graph to two. Hence, like alias coverage,
interleaving segments of size 2 contains a single interleaving 
between two instructions where at least one of them is a write instruction.
%
Using the emulated alias coverage, we run \sys to see whether each
concurrency bug is triggered until the emulated alias coverage is
saturated.
% as a
% feedback of the \sys's speculative interleaving exploration, we run
% \sys with a fixed pair of system calls that can cause each concurrency
% bug, and measure two things: 1) the number of executions until each
% coverage metric is saturated, and 2) whether a given concurrency bug
% is triggered or not.


\PP{Result}
%
\begin{table}[t]
  \small
  \centering
  \input{table/aliascoverage}
  \caption{Trade-off between the bug-finding capability and the search
    complexity. Out of 10 trials using the less-informative
    interleaving coverage metric, \checkmark\xspace indicates that a
    bug is triggered, while - indicates that a bug is not
    triggered. The \texttt{Avg. exec.}  column denotes that the
    average number of execution until the saturation of interleaving
    coverage.}
  \label{table:aliascoverage}
  \vspace{-8pt}
\end{table}
%
\autoref{table:aliascoverage} shows the result.
%
When using emulated alias coverage, \textit{many (six ouf of nine)
concurrency bugs are not discovered even if emulated alias coverage
completely is saturated}.
%
The example of CVE-2017-17712 described in \autoref{s:motivation}
(\ie, \autoref{fig:cve-2017-17712}) explains why such results come 
out; alias coverage considers only interleavings between two
instructions, which is \textit{not sufficient} to execute the
offending interleaving containing multiple instructions. We manually 
identify that all
listed concurrency bugs are caused by thread interleavings of three or
four instructions, which supports our design choice on the size of
interleaving segments (\autoref{ss:overview}).
%
On the other hand, alias coverage is saturated quickly. With
our mutation-based interleaving exploration, it is saturated after 13.9
executions on average, ranging from 6 to 32.
%

\PP{Bug-finding capability and search complexity trade-off}
The result indicates the trade-off between the bug-finding
capability and the search complexity; Alias coverage shows the
\textit{lower} search complexity while it also shows the \textit{weaker}
bug-finding capability.
%
Whereas, using interleaving segment coverage, \sys can discover all listed
concurrency bugs before the saturation of interleaving
segment coverage. Thus, interleaving segment coverage shows the
\textit{stronger} bug-finding capability but it sets the larger search 
space than the alias coverage.
Hence, interleaving segment coverage entails the cost of the
higher search complexity. Nevertheless, we claim that 
its search complexity is still tractable to perform the mutation-based 
interleaving exploration, and we validate our claim  in the next
evaluation~(\autoref{sss:interleavingsearch}).


% that interleaving segment coverage is not saturated until a
% given concurrency bug is triggered (\ie, showing the strong
% bug-finding capability), while less-informative interleaving coverage
% (\eg, alias coverage) is saturated quickly (\ie, showing the low
% search complexity).




%
% As a consequence, we conclude that a fuzzer can rely on interleaving
% segment coverage when determining whether to stop probing new
% interleavings.




%






\subsubsection{Effectiveness of mutation-based interleaving
  exploration}
\label{sss:interleavingsearch}
%
% \begin{figure*}[t]
%   \centering
%   \includegraphics[width=\linewidth]{fig/comparison_graph-crop.pdf}
%   \caption{\dr{table is better?}}
%   \label{fig:eval:comparison}
% \end{figure*}
%
% \begin{table*}
%   \centering
%   \input{table/comparison-interleaving-search.tex}
%   \caption{Result of the comparison study against various
%     state-of-the-art kernel concurrency fuzzers (\ie,
%     KRACE~\cite{krace} and Snowboard~\cite{snowboard}). We measure the
%     number of executions and the elapsed time (secs) required to
%     discover each concurrency bug. The \texttt{Naive} column indicates
%     the kernel's scheduler (\ie, no thread scheduling control applied).}
%   \label{table:comparison-interleaving-search}
%   \vspace{-5pt}
% \end{table*}

% \begin{figure}[t]
%   \centering
%   \includegraphics[width=0.99\linewidth]{fig/comparison_graph_execution-crop.pdf}
%   \caption{Number of executions required for each fuzzer to discover
%     concurrency bugs listed in \autoref{table:knownbugs}. The
%     \texttt{Naive} column indicates the kernel's scheduler (\ie, no
%     thread scheduling control applied).}
%   \label{fig:eval:comparison-execution}
%   \vspace{-8pt}
% \end{figure}

% \begin{figure}[t]
%   \centering
%   \includegraphics[width=0.99\linewidth]{fig/comparison_graph_time-crop.pdf}
%   \caption{Elaped time required for each fuzzer to discover
%     concurrency bugs listed in \autoref{table:knownbugs}.}
%   \label{fig:eval:comparison-time}
%   \vspace{-8pt}
% \end{figure}

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.99\linewidth]{fig/comparison_graph_execution-crop.pdf}
  \caption{Result of the comparison study against various
    state-of-the-art kernel concurrency fuzzers (\ie,
    KRACE~\cite{krace} and Snowboard~\cite{snowboard}). We measure the
    number of executions and the elapsed time (secs) required for each
    fuzzer to discover given concurrency bugs. The \texttt{Naive} column
    indicates the kernel's scheduler (\ie, no thread scheduling
    control applied).}
  \label{fig:eval:comparison}
  \vspace{-5pt}
% \subfloat[The number of executions.]{\includegraphics[width=0.5\linewidth]{fig/comparison_graph_execution-crop.pdf}}
% \subfloat[The elapsed time.]{\includegraphics[width=0.5\linewidth]{fig/comparison_graph_time-crop.pdf}}\\
\end{figure*}

%
One could argue that even if interleaving segment coverage is
informative to describe offending thread interleavings, 
its search space is too large to explore.
%informative in describing unique behaviors of offending thread
%interleavings, its search space is too large to explore, and thus, it
%is impractical.
%
However, it is tractable to run the mutation-based interleaving exploration
using \intcov.
To demonstrate this, we compare the mutation-based interleaving
exploration~(\autoref{ss:scheduler}) against various interleaving
exploration methods proposed in prior approaches.

\PP{Comparison target}
%
We compare \sys to state-of-the-art kernel concurrency fuzzers,
Snowboard~\cite{snowboard}, and KRACE~\cite{krace}.
%
Unfortunately, we cannot directly run Snowboard and KRACE for the
comparison study; KRACE is implemented only for file systems. And
Snowboard runs based on the binary translation (\ie, TCG) of QEMU
without utilizing the hardware acceleration (\ie, KVM), and thus, its
current implementation is not suitable the elapsed time comparison.
%
Therefore, we implement their approaches on the multi-thread fuzzing
phase~(\autoref{sss:multithreadfuzzing}) of \sys by applying 1) the
random delay injection scheme of KRACE, and 2) the
enforcing-single-interleaving-order scheme used by Snowboard.
%
In addition to them, we also compare with the kernel scheduler 
(\ie, no thread scheduling control applied) as the naive baseline.

% %
% Arguably, the most straightforward metric to compare fuzzing
% techniques is the elapsed time until concurrency bugs are discovered.
% %
% However, the elapsed time heavily depends on the randomness;
% \dr{because a fuzzer generates an input program very randomly, it is
%   possible that one fuzzer quickly generates an input program that
%   causes a concurrency bug, while another fuzzer takes very long time
%   to generate the input program}.

% In order to minimize the impact of the randomness and to concentrate
% on the performance impact of scheduling mechanisms, we predefine a
% multi-thread input as shown in \autoref{fig:multithreadinput}, and let
% a fuzzer repeatedly execute the given multi-thread input without
% generating new inputs nor mutating syscalls in the given input.

% In addition, previous fuzzing approaches

\PP{Result}
%
\autoref{fig:eval:comparison} shows the comparison result.
%
In these figures, \texttt{\sys}, \texttt{Snowboard}, and
\texttt{KRACE} display the number of executions and the elapsed time
taken by their corresponding works, and \texttt{Naive} corresponds to
the kernel scheduler without any thread scheduling control enabled.
%
%
In \texttt{Naive}, we can identify the difficulty of discovering
varies from a bug to a bug.
%
For example, CVE-2018-12232 appears as the easiest concurrency bug to
discover since it can be discovered within 78 executions even with the naive kernel scheduler.
%
On the other hand, the kernel scheduler fails to discover three
concurrency bugs, CVE-2019-6974, CVE-2019-11486, and
\texttt{69e16d01d1de} within 10K times of executions.
%
Regardless of the varying difficulty, \sys can discover all of
concurrency bugs within a short time.
%
\sys can discover given concurrency bugs within just 26.8 runs
(ranging from 3 to 81) and 13 seconds (ranging from 1 to 52) on
average.
%
Whereas, Snowboard discovers concurrency bugs within 89 runs (ranging
from 8 to 229 runs) and 37.5 seconds (ranging from 2 to 139) on
average, and KRACE discovers them, if successful, within 329.1 runs
(ranging from 9 to 1296) and 107 seconds (ranging from 4 to 359) on
average.  Moreover, KRACE even suffers from discovering CVE-2019-6974
and \texttt{69e16d01d1de}.

In summary, we confirm that the \sys's mutation-based interleaving
exploration exhibits superior performance by quickly navigating the
search space even if \intcov constructs more complex search space than
alias coverage.

\subsection{Performance characteristics of \sys}
\label{ss:characteristics}

In this subsection, we analyze various performance characteristics of
\sys to comprehend how our approach affects the fuzzing process.
%
\subsubsection{All-inclusive evaluation}
\label{sss:allinclusive}

Here, we provide performance characteristics of the whole \sys.


\PP{Coverage growth.}
%
\begin{figure}[t]
  \centering
  \includegraphics[width=0.9\linewidth]{fig/coverage_graph-crop.pdf}
  \caption{Coverage growth of \sys.}
  \label{fig:eval:coverage}
  \vspace{-8pt}
\end{figure}
%
Since coverage metrics are the paramount performance metric of
fuzzing, we measure the coverage growth for both code coverage (\ie,
the number of taken branches) and interleaving coverage (\ie, the
number of observed interleaving segments) during 100 hours of fuzzing.

As shown in \autoref{fig:eval:coverage}, there is a notable difference
in scale between code coverage (a line denoted by \texttt{Branch}) and
interleaving coverage (a line denoted by \texttt{Interleaving segment}).
%
While the number of taken branches just reaches to 300K, the number of
interleaving segments is over 20M. Thus, the scale of interleaving
coverage is more than 66 times of code coverage.
%
As a consequence, storing interleaving segment coverage consumes more
memory than storing branch coverage.
%
In our implementation, both branch coverage and interleaving segment
coverage are represented as hash tables where each element is 8
bytes. Therefore, storing interleaving segment coverage consumes about
180\MB while storing branch coverage requires 2\MB.
%
We can consider this result as a well-known space-time
tradeoff~\cite{spacetimetradeoff}; we invest \textit{more memory} to
discover concurrency bugs \textit{faster}.
%
% In addition, considering each VM is equipped with 8\GB memory, it is
% endurable for the fuzzer to store interleaving segment coverage using
% 180\MB (or even using ten times of 180\MB).


\PP{Fuzzing throughput.}
%
\begin{table}[t]
  \small
  \centering
  \input{table/throughput}
  \caption{Fuzzing throughput (\# of exec/s) of \sys and
    \texttt{Syzkaller}. \texttt{Syzkaller-memtrace} indicates
    throughput of \texttt{Syzkaller} with memory access tracing
    enabled.}
  \label{table:throughput}
  \vspace{-5pt}
\end{table}
%
All \sys's mechanisms provide benefits in finding concurrency bugs
with a cost of additional overheads and throughput degradation.
%
To comprehend the trade-off, we measure the fuzzing throughput of \sys
and compare it with the \texttt{Syzkaller}'s throughput.
%
In order to experiment in the same environment, we measure throughput
with an empty set of seed. And because both \texttt{Syzkaller} and
\sys restart VMs after an hour of fuzzing, we measure throughput in an
hour of execution in order to eliminate noises caused by, for example,
VM rebooting or kernel crashes.



\autoref{table:throughput} shows the result. As expected, \sys shows
the lower throughput than \texttt{Syzkaller}. In particular, the
\sys's throughput is about 54\% of the \texttt{Syzkaller}'s
throughput.
%
To further understand why the \sys's throughput is degraded, we
additionally measure throughput of \texttt{Syzkaller} while tracing
memory accesses (through instrumentation described in
\autoref{ss:instrumentation}), but not making use of it.
%
As shown in the \texttt{Syzkaller-memtrace} column in
\autoref{table:throughput}, it shows the throughput similar to that of
\sys; the \texttt{Syzkaller-memtrace}'s throughput is just 4.1\%
higher than the throughput of \sys.
%
These results indicate that the throughput of \sys is mainly degraded
by the heavy instrumentation to trace memory accesses.
%
However, as KRACE~\cite{krace} states, it can be understandable as the
cost for the high input quality.
%
In the fuzzer's perspective, while tracking memory accesses has
negative impacts on throughput, it provides a higher chance for a
fuzzer to execute more interesting inputs (\ie, interesting thread
interleaving) and not to waste computing resources.
%
The effectiveness of high input quality is more pronounced in
\autoref{sss:interleavingsearch}, showing \sys can discover
concurrency bugs very quickly.




\PP{Per-input overhead}
%
\begin{table}[t]
  \centering
  \input{table/elapsedtime.tex}
  \caption{
    %
    Elapsed time (ms) for executing one multi-thread input. We
    decompose the elapsed time into the system call execution
    (\texttt{Exec. syscall}), \sys's computational overheads
    (\texttt{Comp. overhead}) and runtime overhead (\texttt{Runtime
      overhead}).}
  \label{table:elapsedtime}
  \vspace{-8pt}
\end{table}
%
In \sys, there are two types of overheads for executing a single
multi-thread input, \ie, computational overheads and runtime
overheads.
%
Specifically, computation overheads are caused by tracking
interleaving segment coverage after executing the
input~(\autoref{ss:coverage}), and calculating scheduling points
before executing the input~(\autoref{ss:scheduler}).
%
On the other hand, runtime overheads are caused by tracing memory
accesses~(\autoref{ss:instrumentation}) and controlling thread
scheduling~(\autoref{ss:engine}).
%
To closely examine these overheads, we measure the elapsed time for
executing a single multi-thread input, and break down the elapsed time
into each overheads.
%
For this measurement, we run 10 thousands times and take an average.




\autoref{table:elapsedtime} shows the result. When executing a single
input, the total elapsed time is 267.2ms.
%
During the execution, the part that took the longest time is executing
system calls; it takes 107.6ms.
%
However, overheads incurred by \sys is not negligible. Tracing memory
accesses~(\autoref{ss:instrumentation}) takes 90.7ms, and controlling
thread scheduling~(\autoref{ss:engine}) takes 42.8ms. These two
runtime overheads almost double the execution time, and are the main
cause of degrading the throughput of fuzzing as shown in the above.
%
In contrast, the total amound of time for computation is 26.1 (= 8.9 +
17.2)ms, and occupies approximately 9\% of the total elapsed time.
%
Accordingly, we can see that the computational overhead is relatively
small.



\subsubsection{Impact on coverage growth}
\label{sss:component}
%
Here, we present the impact of our design choices on both the
interleaving coverage growth and the code coverage growth.


\PP{Impact on interleaving exploration}
%
As the primiary purpose of this work is to effectively explore thread
interleavings, the \sys's approach~(\autoref{s:design}) should
contribute in effectively expanding the interleaving coverage.
%
To see how much \sys improves thread interleaving exploration, we
disable the thread scheduling control in the multi-thread fuzzing
phase of \sys, and measure the interleaving coverage.
%
The result is described in a line denoted by \texttt{Interleaivng
  segment w/o scheduling control} in \autoref{fig:eval:coverage}.
%
With the thread scheduling control disabled, \sys finds 29.1\% less
interleaving segment coverage during the same period.
%
As a consequence, we can concolude that our design choices
significantly improve in exploring thread interleavings.
%
% As a consequence, \sys can effectively discover concurrency bugs as
% shown in \autoref{ss:comparison}.



\PP{Impact on code exploration}
%
Since \sys invests the computing power to repeatedly execute a
multi-thread input (\ie, the multi-thread fuzzing phase), we expect
that \sys might explore code coverage less than the baseline
\texttt{Syzkaller}.
%
To see the difference of the code coverage exploration in \sys and
\texttt{Syzkaller}, we measure code coverage of \texttt{Syzkaller} and
illustrate it as a line denoted by \texttt{Branch (Syzkaller)} in
\autoref{fig:eval:coverage}.
%
As a result, \sys finds 3.2\% less code coverage compared to the
baseline \texttt{Syzkaller}.
%
While this is a definitely downside of \sys. However, considering the
huge benefit in exploring thread interleavings, we still believe that
this is marginal.





%%% Local Variables:
%%% mode: latex
%%% TeX-master: "p"
%%% End:
