\section{Design of \sys}
\label{s:impl}

\begin{figure}
  \centering
  \includegraphics[width=0.95\linewidth]{fig/architecture.pdf}
  \caption{An overview of \sys's architecture.}
  \label{fig:workflow}
  \vspace{-5pt}
\end{figure}

\sys is a kernel fuzzer tailored to effectively discover concurrency
bugs.  Its key improvements lie in adopting interleaving segment
coverage (\autoref{ss:coverage}) and the mutation-based interleaving
exploration (\autoref{ss:scheduler}).
%
As shown in \autoref{fig:workflow}, \sys consists of two stages of
fuzzing.
%
The first stage is a \textit{single-thread fuzzing}, which searches
execution paths similar with conventional fuzzing, and the second
stage is a \textit{multi-thread fuzzing} to explore thread
interleavings.
%
In both stages, \sys traces timestamp-annotated memory accesses
executed by each system call, and in the second stage, \sys adopts a
mechanism to control thread scheduling.



In the following, we provide the overall design of
\sys~(\autoref{ss:fuzzer}).
%
Then, we describe the kernel
instrumentation~(\autoref{ss:instrumentation}) to trace
timestamp-annotated memory accesses, and the execution
engine~(\autoref{ss:engine}) to control thread scheduling.
%
Lastly, we explain the implementation detail of
\sys~(\autoref{ss:impl}).




\subsection{Userspace Fuzzer}
\label{ss:fuzzer}

\sys's single-thread fuzzing~(\autoref{sss:singlethreadfuzzing}) and
multi-thread fuzzing~(\autoref{sss:multithreadfuzzing}) consist of two
components, an input generator and an input executor. We explain
each details in the following.


\subsubsection{Single-thread fuzzing}
\label{sss:singlethreadfuzzing}
%
In a single-thread fuzzing stage, the \textit{single-thread generator}
produces a single-thread input (referred as $I_{ST}$) in the form of a
sequence of random system calls.
%
And then, the \textit{single-thread executor} runs $I_{ST}$ to
identify two system calls in $I_{ST}$ that potentially exhibit new
interleaving segment coverage.  If identified, $I_{ST}$ will be passed
to the next stage, the multi-thread fuzzing.


\PP{Single-thread generator}
%
Similar with a conventional fuzzing, the single-thread generator
constructs a single-threaded system call sequence $I_{ST}$ with two
strategies, generation and mutation.
%
When using the generation strategy, \sys randomly generates a system
call sequence based on well-formed system call description grammar
\texttt{Syzlang}~\cite{syzlang}.
%
\texttt{Syzlang} describes templates of available system calls
including types of arguments, a range of feasible values of each
argument, and the type of a return value.
%
With \texttt{Syzlang}, \sys produces a single-thread system call
sequence by repeatedly selecting a random system call and providing
reasonable arguments of the system call.
%
The mutation strategy is an alternative of the generation strategy.
When using a mutation strategy, \sys picks up an already-generated
single-thread input, and modifies the single-thread input by appending
additional system calls, removing existing system calls, or changing
values of arguments of existing system calls.


\PP{Single-thread executor}
%
Given $I_{ST}$ from the single-thread generator, the single-thread
executor runs $I_{ST}$ while tracing basic blocks and
timestamp-annotated memory accesses executed by each system call.
%
After the execution is finished, the single-thread executor computes
branch coverage using traced basic blocks.
%
If $I_{ST}$ exposes new branch coverage that has not been explored,
\sys keeps $I_{ST}$, and feeds it back to the single-thread generator
so that the single-thread generator further mutates $I_{ST}$ to find
more branch coverage.


Moreover, the single-thread executor identifies a pair of system calls
in $I_{ST}$ that potentially exposes new interleaving segment coverage
if executed concurrently.
%
Specifically, for each system call pair ($S_i$, $S_j$) in $I_{ST}$,
the single-thread executor collects memory accesses executed by the
two system calls. Then, with these memory accesses, the single-thread
executor computes a set of explored segment graphs $G$ (as described
in \autoref{ss:coverage}), and checks $G$ contains new segment graphs
that have not been explored.
%
If so, the single thread executor passes $I_{ST}$ as well as ($S_i$,
$S_j$) and $G$ to the next phase, the multi-thread fuzzing.


\PP{CVE-2017-17712 in single-thread fuzzing}
%
Let us explain how \sys discovers CVE-2017-17712 demonstrated in
\autoref{fig:cve-2017-17712}.
%
Assume the single-thread generator produces $I_{ST}$ as a sequence of
three system calls \texttt{r0 = socket()}, \texttt{setsocktopt(r0)},
\texttt{sendmsg(r0)} as described in \autoref{fig:workflow}.
%
Then, the single-thread executor runs this $I_{ST}$, and collects
memory accesses executed by two system calls \texttt{setsockopt(r0)}
and \texttt{sendmsg(r0)}.
%
Since these memory accesses are annotated with timestamps, the
single-thread executor identifies that all memory accesses executed by
\texttt{setsockopt(r0)} are followed by memory accesses executed by
\texttt{sendmsg(r0)}, and computes a set of segment graphs $G$, which
includes
($\texttt{B1} \Rightarrow \texttt{A2} \Rightarrow \texttt{A4}$) (\ie,
\autoref{fig:alias-coverage}-(a)).
%
Assuming
($\texttt{B1} \Rightarrow \texttt{A2} \Rightarrow \texttt{A4}$) has
not been explored before, \sys determines that it is worth exploring
interleavings between the two system calls.
%
Consequently, the single-thread executor passes $I_{ST}$ as well as
the two system calls and $G$ to the multi-thread fuzzing phase.



% It is worth noting that $S_i$ and $S_j$ were executed
% \textit{sequentially}.
%
% However, the single-thread executor still can
% generate interleaving segment graphs from $S_i$ and $S_j$ in a form
% similar to \autoref{fig:interleavingsegmentgraph}. \dr{TODO: rewrite}



% Instead, the single-thread executor makes use of $G$ to derive
% a set of \textit{unobserved} interleaivng segment graphs
% $G^{derived}$.
% %
% To this end, for each interleaving segment graph $g \in G$,
% the single-thread executor derives new interleaving segment graphs
% (refer to \autoref{fig:interleavingmutation}), and puts ones that
% unobserved before in $G^{derived}$.
% %
% Lastly, if $G^{derived}$ is not empty, the single thread executor
% passes $I_{ST}$ as well as ($S_i$, $S_j$) and $G^{derived}$ to the
% next phase, the multi-thread fuzzing.


% The single-thread executor does not record $G$ in the
% interleaving segment coverage because $S_i$ and $S_j$ were executed
% sequentially.


\subsubsection{Multi-thread fuzzing}
\label{sss:multithreadfuzzing}
%
After $I_{ST}$ is passed with ($S_i$, $S_j$) and $G$, the
\textit{multi-thread generator} transforms $I_{ST}$ to a multi-thread
input $I_{MT}$.
%
In addition, the multi-thread generator produces \textit{schedules},
each of which contains a set of scheduling points.
%
The \textit{multi-thread executor} then repeatedly tests each schedule
of $I_{MT}$ with a support of the execution
engine~(\autoref{ss:engine}).


% \PP{Schedule}
% %
% A schedule is a specification of how to control thread scheduling.
% %
% It contains a starting system call and per-system call sequences of
% scheduling points that each scheduling point is described as a
% two-tuple (instruction address, scheduling order).

% \begin{figure}[t]
%   \centering
%   \includegraphics[width=0.75\linewidth]{fig/schedule.pdf}
%   \caption{(a) Thread interleaving, and (b) schedule to run thread
%     interleaving of (a). \dr{remove}}
%   \label{fig:schedule}
% \end{figure}

% \autoref{fig:schedule} illustrates an example of a schedule. In order
% to run a thread interleaving in \autoref{fig:schedule}-(a), the
% execution should start with \texttt{sendmsg()}. So the schedule states
% that a starting syscall is \texttt{sendmsg()}.
% %
% Then, in \autoref{fig:schedule}-(a), three preemptions (excluding the
% end of system calls) are required at \texttt{A2}, \texttt{B1}, and
% \texttt{A4}. Each schedule point denotes an instruction on which a
% preemptions occurs, annotated with the order of preemption.


%During fuzzing, the userspace fuzzer keeps generating different
%schedules, and requests the execution engine through hypercall
%interfaces to control thread scheduling according to a schedule.




\PP{Multi-thread generator}
%
% In order to use interleaving graph as interleaving coverage, we need a
% method to store them, and compare them to a new interleaving graph.
% %
% We choose to use a hash value of
%
% the FNV-1~\cite{fnv, fnv-go}, non-cryptographic hash function,
%
%
% A schedule is an outcome of the interleaving mutation.
% A schedule contains an initial thread, and a set of scheduling points
% indicating an instruction on which preemption occurs.
%
The multi-thread generator has two roles, 1) transforming $I_{ST}$
into a multi-thread input $I_{MT}$ and 2) generating various
schedules from a set of explored segment graphs $G$.
%
% \begin{figure}[t]
%   \scriptsize
%   \centering
%   \input{code/gen-mt-rcprog.py}
%   \caption{\sys's multi-thread generator algorithm. \dr{Seems too
%       much}}
%   \label{f:mt-conversion}
% \end{figure}
Given $I_{ST}$ and ($S_i$, $S_j$), the multi-thread generator first
produces $I_{MT}$, which preserves all system calls in $I_{ST}$, but
executes $S_i$ and $S_j$ concurrently.
%
Assuming $i < j$, the input transformation is simply done by dividing
$I_{ST}$ into two parts.
%
The first part contains system calls from the 1st to the i-th, and the
second part contains system calls from the (i+1)-th to the last.
%
Later, these two parts will be executed by two different threads,
while $S_i$ and $S_j$ will be executed concurrently. A detailed
example is provided in \autoref{s:appendix:inputtransform}.



On the other hand, the multi-thread generator repeatedly produces
various schedules.
%
To generate schedules, the multi-thread generator implements the
mutation-based interleaving exploration method detailed in
\autoref{ss:scheduler}.
%
Briefly explaining, given a set of explored segment graphs $G$, the
multi-thread generator mutates segment graphs in $G$ to derive a set
of unexplored mutated segment graphs $G_{mutated}$.
%
Then, the multi-thread generator selects a subset of $G_{mutated}$
called $G^{*}_{mutated}$, and forms a schedule for testing mutated
segment graphs contained in $G^*_{mutated}$.
%
Lastly, the generated schedule and $I_{MT}$ is passed to the
multi-thread executor.






\PP{Multi-thread executor}
%
The multi-thread executor runs $I_{MT}$ while enforcing a schedule
generated by the multi-thread generator.
%
During the execution of $I_{MT}$, the multi-thread executor checks
that the running thread interleaving causes a harmful behavior (\eg,
memory corruption or deadlock) with a support from devloper tools such
as lockdep~\cite{lockdep}, kernel watchdog~\cite{watchdog}, and
sanitizers~\cite{kasan, ubsan, asan, meds}.
%
If these developer tools detect that an abnormal behavior occurs in
the kernel, the multi-thread executor writes a report of the abnormal
behavior as well as $I_{MT}$ and the executed thread interleaving.







Otherwise, the multi-thread executor collects memory accesses that
$S_i$ and $S_j$ executed.
%
With these memory accesses, the multi-thread executor probes for
\intcov as described in \autoref{ss:coverage} by decomposing the
executed interelaving into another set of segment graphs $G'$. In
addition, the multi-thread executor feeds $G'$ to the multi-thread
generator, allowing the multi-thread generator to further explore
thread interleavings.



\PP{CVE-2017-17712 in multi-thread fuzzing}
%
Once receiving $I_{ST}$, the multi-thread generator transforms
$I_{ST}$ into $I_{MT}$. In this example, thread~A executes two system
calls, \texttt{r0 = socket{}} and \texttt{setsockopt(r0)}, and
thread~B executes one system call \texttt{sendmsg(r0)}, while two
system calls \texttt{setsockopt(r0)} and \texttt{sendmsg(r0)} will be
executed concurrently (as described as $I_{MT}$ in
\autoref{fig:workflow}).
%
Furthermore, the multi-thread generator produces a schedule according
to the mutation-basd interleaving exploration. Given $G$ including
($\texttt{B1} \Rightarrow \texttt{A2} \Rightarrow \texttt{A4}$), the
multi-thread generator produces a mutated segment graph corresponding
to ($\texttt{A2} \Rightarrow \texttt{B2} \Rightarrow \texttt{A4}$)
(refer \autoref{fig:interleavingmutation}). Then, the multi-thread
generator generates a schedule to test
($\texttt{A2} \Rightarrow \texttt{B1} \Rightarrow \texttt{A4}$) (refer
\autoref{fig:hint}).
%
Lastly, when the multi-thread executor runs $I_{MT}$ with the
generated schedule, the uninitialized access bug is triggered.



\subsection{Kernel Instrumentation}
\label{ss:instrumentation}

\sys requires to trace basic blocks (for tracking code coverage) and
timestamp-annotated memory accesses (for tracking interleaving
coverage) executed by each system call.
%
For this, \sys incorporates a compiler pass that inserts callback
function calls 1) at the beginning of each basic block, and 2) before
each instruction that accesses globally-visible memory objects.
%
% These callback functions record executed basic blocks and memory
% accesses into per-thread memory regions shared between the user space
% and the kernel space. Thus after a thread executes a system call, the
% thread can identify basic blocks and memory accesses that it executed.



\begin{figure}
  \centering
  \includegraphics[width=0.9\linewidth]{fig/instrumentation.pdf}
  \caption{(a) Part of \texttt{raw_sendmsg()} in
    \autoref{fig:cve-2017-17712}, (b) instrumented source code of (a)
    (gray on the background), and (c) memory accesses and basic blocks
    recorded during runtime.}
  \label{fig:instrumentation}
  \vspace{-5pt}
\end{figure}

\autoref{fig:instrumentation} shows how the \sys's compiler pass
inserts callbacks, and how each callback records memory accesses and
basic blocks.
%
When compiling a source code in (a), the compiler pass inserts two
function calls; \texttt{trace_basicblock()} before \texttt{A3} which
is the first instruction of the basic block, and
\texttt{trace_access()} before \texttt{A2} which is an instruction
that accesses \texttt{inet->hdrincl}.
%
During runtime, \texttt{trace_basicblock()} takes the starting address
of basic block as input (\ie, \texttt{A3}), and records it into
\texttt{Basicblock record}.
%
On the other hand, \texttt{trace_access()} takes five parameters,
addresses of the accessed memory object and the instruction, the size
and the type of the memory access, and the timestamp when the memory
access occurs (\ie, \texttt{\&inet->hdrincl}, \texttt{A2}, \texttt{1},
\texttt{read}, and \texttt{ts}).
%
\texttt{trace_access()} then records these five parameters into
\texttt{Memory access record}.
%
During fuzzing, the userspace fuzzer reads values recorded in
\texttt{Basicblock record} and \texttt{Memory access record}, and
identifies basic blocks and memory accesses executed by each system
call.





% It is worth noting that \texttt{Basicblock record} and \texttt{Memory
%   access record} are per-thread memory regions, shared between the
% user space and the kernel space. Thus, after a userspace thread
% executes a system call, the thread can identify basic blocks and
% memory accesses that it executed.







% Unlike data race detectors such as KCSAN~\cite{kcsan}, the \sys's
% scheduling mechanism needs to recognize both plane memory accesses and
% annotated memory accesses such as atomic operations.
% %
% We deal with these two types of accesses differently since annotated
% accesses usually are implemented in assembly code, which is hard for a
% LLVM pass to understand.
% %
% In order to instrument plane accesses, we implement a LLVM pass that
% insert callback function calls after memory accesses on LLVM IR.
% %
% Our pass runs after most of binary transfomration is done, so it
% \XXX{...}.
% %
% For annotated instructions, we rely on the functionality of
% KASAN~\cite{kasan} to instrument atomic operations.
% %
% KASAN provides wrapper functions of annotation APIs to call callback
% functions before annotated memory operations, and we instruct the
% wrapper functions to call our callbacks as well.
% %
% Our callbcak functions write memory access operations along with
% various information into a region mmap-ed shared region shared by a
% userspace program (\ie, fuzzer) and a kernel. The information about
% memory operations includes the instruction address, the start address
% of a memory location, and the size of memory access.
% %
% Accordingly, a fuzzer is able to identify what memory access
% operations took place during the execution.

% \sys also requires an additional module called a trampoline that is
% used to suspend and resume a running thread. Details about the
% trampoline are described in \autoref{ss:engine}.


\subsection{Execution Engine}
\label{ss:engine}

\sys introduces an execution engine in order to grant an ability to
the userspace fuzzer that it can control thread scheduling.
%
The execution engine is implemented in the hypervisor layer to be
non-intrusive to the kernel execution.




% In order to monitor the kernel execution, \sys leverages the hardware
% breakpoint feature~\cite{hwbp} shipped in modern Intel CPU chipsets.
% %
% The execution engine installs breakpoints on instructions on which
% scheduling points refer, and recognizes that a thread of the userspace
% fuzzer reaches a scheduling point when a breakpoint is hit.
%


\PP{Enforcing a schedule}
%
In order to request the execution engine to enforce a schedule, the
userspace fuzzer sends a schedule to run through hypercall interfaces
before executing system calls.
%
After that, the threads notify the execution engine that they are
ready to execute system calls, and start executing system calls.
%
During the execution, the execution engine deliberately suspends and
resumes the execution of system calls as described in the given
schedule.

%
\begin{figure}[t]
  \centering
  \includegraphics[width=0.99\linewidth]{fig/workflow-hypervisor.pdf}
  \caption{The workflow of the execution engine.}
  \label{fig:workflow-hypervisor}
  \vspace{-5pt}
\end{figure}
%
\autoref{fig:workflow-hypervisor} shows a workflow of the execution
engine.
%
At the beginning, the userspace fuzzer spawns threads, where each
thread is assigned a system call and scheduling points (\C{1} in
\autoref{fig:workflow-hypervisor}).
%
Then, each thread invokes hypercalls (\ie, \texttt{hcall_sched()}) to
deliver scheduling points to the execution engine (\ie, \C{2}). When
the execution engine receives a scheduling point, the execution engine
installs a breakpoint on an instruction on which the scheduling point
refers (\ie, \C{2}-1).
%
It is worth noting that \texttt{hcall_sched()} takes the order of
scheduling points as a second paramter. In this example, preemption
will occur first on \texttt{A2}, followed by \texttt{B2} and
\texttt{A6} in order during the execution.
%
After delivering scheduling points, each thread calls
\texttt{hcall_ready()}, which sleeps until all threads call are ready,
and after all threads call \texttt{hcall_ready()}, the execution
engine wakes them up, and the threads start executing system calls
(\ie, \C{3}).
%
While executing system calls, the execution engine keeps only one
thread to run. When the running thread reaches a scheduling point, the
execution engine performs preemption (\ie, \C{4}) by suspending the
running thread and resuming the next thread to run as specified in the
given schedule (\ie, \C{4}-1).




% \dr{I'm rewriting this part to shrink}

% \PP{Enforcing a schedule}
% %
% In order for the userspace fuzzer to ask the execution engine to
% enforce a schedule, the userspace fuzzer sends a schedule to run
% thorugh hypercall interfaces.
% %
% After that, the threads notify the execution engine that they are
% ready to execute system calls, and start executing system calls,
% %
% During the execution, the execution engine keeps only one thread to
% run, and deliberately suspends and resumes the execution of system
% calls according to the given schedule.

% %
% \begin{figure}[t]
%   \centering
%   \includegraphics[width=0.9\linewidth]{fig/workflow-hypervisor.pdf}
%   \caption{The workflow of the execution engine. \red{IMPORTANT: this
%       is copied from AITIA. Need to redraw}}
%   \label{fig:workflow-hypervisor}
% \end{figure}
% %
% \autoref{fig:workflow-hypervisor} shows a workflow of the execution
% engine.
% %
% At the beginning, the userspace fuzzer spawns threads, where each
% thread is assigned a system call annotated with scheduling points
% (\C{1} in \autoref{fig:workflow-hypervisor}).
% %
% Then, each thread invokes hypercalls to deliver scheduling points to
% the execution engine (\C{2}). In \sys, three types of hypercalls are
% used; \texttt{hcall_XXX()} to inform the starting system call,
% \texttt{hcall_sched()} to deliever a single scheduling point, and
% \texttt{hcall_ready()} to notify that a calling thread is ready.
% %
% Also, \texttt{hcall_ready()} sleeps until all threads all threads are
% ready. When all threads invoke \texttt{hcall_ready()}, the execution
% engine wakes them up, and the threads start executing system calls
% (\C{3}).

% \dr{installing BPs}


% During the execution of system calls, only one thread is allowed to
% run. In \autoref{fig:workflow-hypervisor}, thread~A (\ie,
% \texttt{sendmsg()}) runs first because it is a starting system
% call (\ie, it called \texttt{hcall_XXX()}).
% %
% Because a breakpoint is installed on \texttt{A2}, the execution engine
% is noticed that thread~A reaches \texttt{A2}.
% %
% Then, the execution engine suspends thread~A, and switches to exeucte
% thread~B.
% %
% To suspend thread~A, the execution enigne stores context information
% (\eg, register values) in the hypervisor's memory, and changes the
% program counter of thread~A to an infinite loop called a trampoline.
% %
% In the trampoline, thread~A keeps calling \texttt{cond_resched()} to
% yield a CPU without proceeding with the system call execution, thus
% thred~A is confined in the trampoline.
% %


% While thread~A is confined in the trampoline, thread~B runs its system
% call \texttt{setsockopt()}.
% %
% But another breakpoint is installed on \texttt{B1}, when thread~B
% reaches a scheduling point (\texttt{B1}), the execution engine again
% suspends thread~B and switches to execute thread~A.
% %
% Suspending thread~B is performed in the same way described above.
% %
% To resume thread~A , the execution engine restores registers with the
% values of registers including a program counter saved when thread~A
% was suspended. Then thread~A can continue its execution.


% The execution engine keeps suspending and resuming system calls until
% one of the two threads finishes its execution. In this example,
% thread~A finishes its execution first, and notifies the end of
% execution to the execution engine through \texttt{hcall_done()}.
% %
% Upon receiving \texttt{hcall_done()}, the execution engine resumes the
% suspended thread (\ie, thread~B) and lets it finish its execution.





\PP{Performing preemption}
%
In order to perform preemption, \sys leverages the hardware breakpoint
feature~\cite{hwbp} shipped in modern Intel CPU chipsets.
%
During the execution, the execution engine recognizes that the
execution reaches a scheduling point when a breakpoint is hit.
%
When a thread hits a breakpoint, the execution engine stores context
information (\eg, register values) of the running thread in the
hypervisor's memory, and changes the program counter of the tthread to
an infinite loop called a trampoline.
%
In the trampoline, the thread keeps calling a kernel API
\texttt{cond_resched()} which yields a CPU. As a consequence, the
thread is suspended in the trampoline without making a progress.
%
To resume the suspended thread, the execution engine restores
registers including a program counter with the values stored when the
thread was suspended, and then the thread can continue its execution.






\PP{Restriction of hardware breakpoint}
%
While the breakpoint feature is well-fitting for our pupose, it has a
restriction on the number of breakpoints that can be installed at the
same time.
%
In Intel CPU chipsets, at most four hardware breakpoints can be
installed simultaneously, and the number of scheduling points can be
larger than four.
%
\sys overcomes this restriction by leveraging the fact that scheduling
points are ordered. Specifically, the execution engine installs
breakpoints on first four scheduling points, and when a installed
breakpoint is hit, the execution engine moves a breakpoint onto the
next scheduling point.
%



%\dr{not important. i'm thinking remove this}
%
%In performing preemption, we choose to suspend and resume a guest
%thread instead of a whole virtual CPU on which the guest thread runs.
%
%While suspending and resuming a whole virtual CPU is adopted by
%previous approaches~\cite{ski, snowboard, razzer} it is not suitable
%for our purpose because \textit{if suspending a virtual CPU, it may
%  suspend another virtual CPU}. Details are explained in the appendix
%section~(\autoref{s:appendix:preemption}).




\PP{Handling missing scheduling points}
%
\dr{TODO: rewrite}
%
It is worth noting that instructions on which scheduling points
install may not be executed if the control flow changes due to kernel
internal states.
%
In such case, the execution engine keeps the order of scheduling
points.
%
\dr{}
For example, when executing a schedule of \autoref{fig:schedule}, it
is possible that thread~A may not execute \texttt{A2}, and hit a
breakpoint on \texttt{A6}.
%
Then, we ignore all scheduling points before \texttt{A6} (\ie,
\texttt{A2} and \texttt{B1}), and keep enforcing a schedule after
\texttt{A6} (\ie, \texttt{B2}).



\PP{Virtual Machine Instrospection (VMI)}
%
The execution engine introspects the target kernel for two reasons.
%
First, because a hardware breakpoint does not distinguish which thread
hits the breakpoint, the execution engine needs to determine whether
the breakpoint is hit by a thread of the userspace fuzzer, or by an
irrelevant thread.
%
If a breakpoint is hit by a thread of the userspace fuzzer, the
execution engine perform preemption as specified in a schedule.
%
Otherwise, the execution engine ignores the breakpoint hit, and keeps
running the kernel.
%
Second, when a running thread tries to acquire a lock, the execution
engine inspects whether the lock is held by a suspended thread, which
may cause the unexpected block.
%
If it is the case, the execution engine performs preemption so that
the lock-holder thread (\ie, the suspended thread) resumes the
execution, and the lock-waiting thread gets suspended.
%
While the VMI is crucial to properly control thread scheduling, we
leave details of how we implement the VMI in the appendix
section~(\autoref{s:appendix:vmi}).




\subsection{Implementation}
\label{ss:impl}

We implement \sys in various software layers as follows:
%
The \sys's userspace fuzzer~(\autoref{ss:fuzzer}) is implemented based
on \texttt{Syzkaller}~\cite{syzkaller}, a kernel fuzzer developed by
Google, with 3334~\footnote{We use scc~\cite{scc} and
  sloccount~\cite{sloccount} to measure LoC of GO and C, C++
  respectively.} LoC in GoLang and 341 LoC in C++.
%
The kernel instrumentation~(\autoref{ss:instrumentation}) is
implemented in two parts, a compiler pass and callback functions. The
compiler pass is implemented on the LLVM compiler suite
12.0.1~\cite{llvm} with 323 LoC in C++, and, the callback functions
are implemented in the Linux kernel source tree with 265 LoC in C.
%
Lastly, the execution engine~(\autoref{ss:engine}) is implemented on QEMU
6.0.0 with 1662 LoC in C, and leverages KVM (Kernel-based Virtual
Machine) to take advantage of hardware acceleration.
%
% In addition, the trampoline is enclosed into the target kernel source
% tree as a loadable module.



% To summarize, the target kernel
% instrumentation~(\autoref{ss:instrumentation}) is implemented with 323
% LoC in C++ (for the compiler pass) and 265 LoC in C (for callback
% functions).
% %
% The execution engine~(\autoref{ss:engine}) is implemented with 1662
% LoC in C. And the userspace fuzzer~(\autoref{ss:fuzzer}) is
% implemented with 3334 LoC in GoLang and 341 LoC in C++.

% the compiler pass is implmented in 323 lines of C++
% the callback functions are implmeneted in 265 lines of C
% the execution engine is implemented in 1662 lines of C
% the trampoline is 66 in C
% c2fuzz:
% go 3334
% C++ 341







%%% Local Variables:
%%% mode: latex
%%% TeX-master: "p"
%%% End:
