\section{Design of \sys}
\label{s:impl}

\sys is a coverage-directed concurrency fuzzer to effectively discover
concurrency bugs.
%
The key improvement of \sys lies in adopting interleaving segment
coverage and the coverage-directed interleaving search strategy
described in \autoref{s:design}.
%

In particular, \sys consists of two stages of fuzzing.
%
The first stage is a single-thread fuzzing, which focuses on expanding
code coverage\dr{}.
%
The second stage is a multi-thread fuzzing to explore thread
interleavings, and discover concurrency bugs.
%
In each stage, \sys requires to profile memory accesses executed by
each syscall to track interleaving segment coverage.
%
And in the the second stage, \sys needs a mechanism to control thread
scheduling as desired.



In the following of subsections, we firstly provide two building
blocks of \sys, the target kernel
instrumentation~(\autoref{ss:instrumentation}) to profile memory
accesses, and the execution engine~(\autoref{ss:engine}) to control
thread schedulings.
%
And then we describe design details of \sys~(\autoref{ss:fuzzer}),
followed by implementation details of \sys~(\autoref{ss:impl}).


\subsection{Target Kernel Instrumentation}
\label{ss:instrumentation}

\sys requires to profile basic blocks (for code coverage) and memory
accesses (for interleaving coverage) executed by each system call.
%
To this end, \sys incorporates a compiler pass that inserts callback
function calls 1) at the beginning of each basic block, and 2) before
each instruction that accesses globally-visible memory objects.
%
These callback functions records executed basic blocks and memory
accesses into memory regions shared between the user space and the
kernel space.



\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{fig/instrumentation.pdf}
  \caption{(a) Part of \texttt{raw_sendmsg()} in
    \autoref{fig:cve-2017-17712}, (b) instrumented source code of (a)
    (gray on the background), and (c) memory accesses and basic blocks
    recorded during runtime.}
  \label{fig:instrumentation}
\end{figure}

\autoref{fig:instrumentation} shows how the \sys's compiler pass
inserts callbacks, and how each callback records memory accesses and
basic blocks.
%
When compiling a source code in (a), the compiler pass two inserts
function calls; \texttt{trace_basicblock()} before \texttt{A3} which
is the first instruction of the basic block, and
\texttt{trace_access()} before \texttt{A2} which is an instruction
that accesses \texttt{inet->hdrincl}.


During runtime, \texttt{trace_basicblock()} takes the starting address
of basic block as input (\ie \\texttt{A3}), and records the starting
address into \texttt{Basicblock record}.
%
On the other hand, \texttt{trace_access()} takes four parameters, the
address of the accessed memory object (\ie, \texttt{\&inet->hdrincl}),
the instruction address accessing the memory object (\ie,
\texttt{A2}), the size of the memory access (\ie, \texttt{1}), and
the type of the memory access (\ie, \texttt{read}), and records them
into \texttt{Memory access record}.






It is worth noting that \texttt{Basicblock record} and \texttt{Memory
  access record} are per-thread memory regions, shared between the
user space and the kernel space. Thus, after a userspace thread
executes a system call, the thread can identify basic blocks and
memory accesses that it executed.







% Unlike data race detectors such as KCSAN~\cite{kcsan}, the \sys's
% scheduling mechanism needs to recognize both plane memory accesses and
% annotated memory accesses such as atomic operations.
% %
% We deal with these two types of accesses differently since annotated
% accesses usually are implemented in assembly code, which is hard for a
% LLVM pass to understand.
% %
% In order to instrument plane accesses, we implement a LLVM pass that
% insert callback function calls after memory accesses on LLVM IR.
% %
% Our pass runs after most of binary transfomration is done, so it
% \XXX{...}.
% %
% For annotated instructions, we rely on the functionality of
% KASAN~\cite{kasan} to instrument atomic operations.
% %
% KASAN provides wrapper functions of annotation APIs to call callback
% functions before annotated memory operations, and we instruct the
% wrapper functions to call our callbacks as well.
% %
% Our callbcak functions write memory access operations along with
% various information into a region mmap-ed shared region shared by a
% userspace program (\ie, fuzzer) and a kernel. The information about
% memory operations includes the instruction address, the start address
% of a memory location, and the size of memory access.
% %
% Accordingly, a fuzzer is able to identify what memory access
% operations took place during the execution.

% \sys also requires an additional module called a trampoline that is
% used to suspend and resume a running thread. Details about the
% trampoline are described in \autoref{ss:engine}.


\subsection{Execution Engine}
\label{ss:engine}

\sys introduces an execution engine in order to grant an ability to
the userspace fuzzer that it can control thread scheduling.
%
The execution engine is implemented in the hypervisor layer to be
non-intrusive to the kernel execution, and the userspace fuzzer can
request the execution engine to control thread scheduling through
hypercall interfaces.


\PP{Schedule}
%
A schedule is a specification of how to control thread scheduling.
%
It contains per-system call sequences of scheduling points, where each
scheduling point is described as a two-tuple, (instruction address,
scheduling order).

\begin{figure}[t]
  \centering
  \includegraphics[width=0.9\linewidth]{fig/schedule.pdf}
  \caption{(a) Thread interleaving to run, and (b) schedule describes
    thread interleaving of (a).}
  \label{fig:schedule}
\end{figure}

\autoref{fig:schedule} illustrates an example of a schedule. In order
to run a thread interleaving in \autoref{fig:schedule}-(a), the
execution should start with \texttt{sendmsg()}. So the schedule states
that a starting syscall is \texttt{sendmsg()}.
%
Then, in \autoref{fig:schedule}-(a), four preemptions (including the
end of system calls) are required at \texttt{A2}, \texttt{B1},
\texttt{A6}, and \texttt{B2}, and each schedule point denotes on which
these preemptions occur, annotated with the order of preemption.


During fuzzing, the userspace fuzzer keeps generating different
schedules, and requests the execution engine through hypercall
interfaces to control thread scheduling according to a schedule.





\PP{Workflow of enforcing a schedule}
%
In order for the userspace fuzzer to ask the execution engine to
enforce a schedule, the userspace fuzzer sends a schedule to run
before executing system calls.
%
After that, the threads notify the execution engine that they are
ready to execute system calls, and start executing system calls,
%
During the execution, the execution engine keeps only one thread to
run, and deliberately suspends and resumes the execution of system
calls according to the given schedule.

%
\begin{figure}[t]
  \centering
  \includegraphics[width=0.9\linewidth]{fig/workflow-hypervisor.pdf}
  \caption{The workflow of the execution engine. \red{IMPORTANT: this
      is copied from AITIA. Need to redraw}}
  \label{fig:workflow-hypervisor}
\end{figure}
%
\autoref{fig:workflow-hypervisor} shows a workflow of the execution
engine.
%
At the beginning, the userspace fuzzer spawns threads, where each
thread is assigned a system call annotated with scheduling points.
%
Then, each thread invokes hypercalls (\ie,
\texttt{hcall_scheduling_point()}) to deliver scheduling points to the
execution engine, followed by a hypercall \texttt{hcall_ready()} that
sleeps until all threads are ready.
%
After all threads invoke \texttt{hcall_ready()}, the execution engine
wakes them up, and the threads start executing system calls.


\dr{}



\PP{Performing preemption}
%
In order to perform preemption, \sys leverages the hardware breakpoint
feature~\cite{hwbp} shipped in modern Intel CPU chipsets.
%
When the execution engine receives scheduling points, the execution
engine installs breakpoints on instructions on which scheduling points
refer, and recognizes that a thread of the userspace fuzzer reaches a
scheduling point when a breakpoint is hit.


When a thread hits a breakpoint, the execution engine saves register
values of the thread, and then changes the program counter of the
thread to an infinite loop called a trampoline.
%
In the trampoline, the thread keeps calling \texttt{cond_resched()} to
yield a CPU without proceeding with the system call execution.
%
To resume the suspended thread, the execution engine restores
registers with the values of registers including a program counter
saved when the thread was suspended, and then the thread can continue
its execution.


\dr{where to put this?}
%
While a hardware breakpoint feature is well-fitting for our pupose, it
has a restriction on the number of breakpoints that can be installed
at the same time.
%
In Intel CPU chipsets, at most four hardware breakpoints can be
installed simultaneously, and the number of scheduling points can be
larger than four.
%
\dr{} \sys overcomes this restriction by leveraging the fact that
scheduling points are ordered. In other words, the execution engine
installed breakpoints on a first few (\eg, four) scheduling points,
and when a installed breakpoint is hit, the execution engine moves a
breakpoint onto the next scheduling point.
%
Also we expect that this restriction can be further mitigated by using
architectures with more hardware breakpoints (\eg, ARM v7 equipped six
hardware breakpoints), or using a software breakpoint feature that
does not have a restriction on the number of breakpoints.



\dr{not important. i'm thinking remove this}
%
In performing preemption, we choose to suspend and resume a guest
thread instead of a whole virtual CPU on which the guest thread runs.
%
While suspending and resuming a whole virtual CPU is adopted by
previous approaches~\cite{ski, snowboard, razzer} it is not suitable
for our purpose because \textit{if suspending a virtual CPU, it may
  suspend another virtual CPU}. Details are explained in the appendix
section~(\autoref{s:appendix:preemption}).




\PP{Handling missing scheduling points}
%
It is worth noting that instructions on which scheduling points
install may not be executed during the execution, for example, in case
that the control flow changes because of kernel internal states.
%
For example, it is possible that

\dr{}




\PP{Virtual Machine Instrospection}
%
While controlling thread scheduling of system calls, the execution
engine introspects the target kernel for two reasons.
%
First, because a hardware breakpoint does not distinguish which thread
hits the breakpoint, the execution engine needs to determine whether
the breakpoint is hit by a thread of the userspace fuzzer, or by an
irrelevant thread.
%
If a breakpoint is hit by a thread of the userspace fuzzer, the
execution engine suspends the thread and resumes the
previously-suspended thread as specified in a schedule.
%
Otherwise, the execution engine ignores the breakpoint hit, and keeps
running the kernel.


Second, when a running thread tries to acquire a lock, the execution
engine inspects whether the lock is held by a suspended thread to
prevent unexpected block (\ie, both threads cannot make a progress).
%
If it is the case, the execution engine performs preemption so that
the lock-holder thread (\ie, the suspended thread) resumes the
execution, and the lock-waiting thread gets suspended.


While the virtual machine introspection is crucial to properly control
thread scheduling, we leave details of how we implement the virtual
machine introspection in the appendix
section~(\autoref{s:appendix:vmi}).



\subsection{Userspace Fuzzer}
\label{ss:fuzzer}



\begin{figure}
  \includegraphics[width=0.9\linewidth]{fig/architecture.pdf}
  \caption{Workflow of \sys \dr{TODO: redraw to describe the workflow}}
  \label{fig:workflow}
\end{figure}


The \sys's userspace fuzzer conducts two-stage pipelined fuzzing
similar with recent concurrency fuzzers, Razzer~\cite{razzer} and
Snowboard~\cite{snowboard}.
%
In \sys, the first stage is a \textit{single-thread} fuzzing that
focuses on expanding code coverage\dr{}, and the second stage is a
\textit{multi-thread} fuzzing to explore thread interleavings.

Both stages consists of two components, an input generator and an
input executor. We explain details of each component in the following.


\subsubsection{Single-thread fuzzing}
%
In a single-thread fuzzing phase, the single-thread generator
generates a single-thread input (refered as $I_{ST}$) in the form of a
sequence of random system calls.
%
And then, the single-thread executor executes $I_{ST}$ to perform two
things.
%
First, it collects code coverage of $I_{ST}$ with the purpose of
exploring source codes residing deep inside the kernel.
%
Second, it identifies two system calls in $I_{ST}$ that potentially
exhibit new interleaving coverage.  If identified, $I_{ST}$ will be
handed to the next phase, namely the multi-thread fuzzing.


\PP{Single-Thread Generator}
%
The single-thread generator is similar with conventional
fuzzing~\cite{syzkaller}.
%
It constructs a single-threaded system call sequence $I_{ST}$ with two
strategies: generation and mutation.
%
When using the generation strategy, \sys randomly generates a system
call sequence based on well-formed system call description grammar
\texttt{Syzlang}~\cite{syzlang}.
%
\texttt{Syzlang} describes templates of available system calls
including types of arguments and the type of a return value, as well
as a range of feasible values of each arguments.
%
According to \texttt{Syzlang}, \sys produces a single-thread system
call sequence by repeatedly selecting a random system call and
providing reasonable arguments of the system call.

The mutation strategy is an alternative of the generating strategy.
When using a mutation strategy, \sys picks up a already-generated
single-thread input, and modifies the single-thread input by appending
additional system calls, removing existing system calls, or changing
values of arguments of existing system calls.


\PP{Single-Thread Executor}
%
Given $I_{ST}$ from the single-thread generator, the single-thread
executor runs $I_{ST}$, and profiles basic blocks and memory accesses
executed by each system call with a support from instrumentation
detailed in \autoref{ss:instrumentation}.

With profiled basic blocks and memory accesses, the single-thread
executor conducts two tasks.
%
The first task is similar with what conventional kernel fuzzing
does.
%
In other words, the single-thread executor computes branch coverage
using profiled basic blocks.
%
Then, if $I_{ST}$ exposes new branch coverage that has not been
explored, \sys keeps $I_{ST}$, and feeds it back to the single-thread
generator so that the single-thread generator further mutates $I_{ST}$
to find more branch coverage.
%
As a result, \sys keeps a minimal set of $I_{ST}$, called a corpus,
which may execute previously-executed branches when running
single-thread inputs in the corpus.


Second, the single-thread executor identifies a pair of system calls
in $I_{ST}$ potentially exposes new interleaving coverage if executed
concurrently.
%
More specifically, for each pair of system calls, the single-thread
executor derives interleaving segment graphs, and checks that among
derived interleaving segment graphs, there is one 
%



\subsubsection{Multi-thread fuzzing}
%
After $I_{ST}$ is handed with a pair of system calls to execute
concurrently, the multi-thread generator transforms $I_{ST}$ to a
multi-thread input $I_{MT}$.
%
In addition, the multi-thread generator generates \textit{schedules}
where each schedule describes how to enforce thread interleaving (\ie,
a set of scheduling points) during runtime.


The multi-thread executor then repeatedly tests each schedule of
$I_{MT}$ one at a time.
%
To this end, the multi-thread executor instruments $I_{MT}$ with
hypercalls in order to tell the execution engine how to control thread
scheduling.
%
Lastly, the multi-thread executor runs $I_{MT}$ while enforcing a
given schedule with a support of the execution
engine~(\autoref{ss:engine}).




\PP{Multi-Thread Generator}
%
% In order to use interleaving graph as interleaving coverage, we need a
% method to store them, and compare them to a new interleaving graph.
% %
% We choose to use a hash value of

% the FNV-1~\cite{fnv, fnv-go}, non-cryptographic hash function,


% A schedule is an outcome of the interleaving mutation.
% A schedule contains an initial thread, and a set of scheduling points
% indicating an instruction on which preemption occurs.


With a given $I_{ST}$ and a pair of system calls, the multi-thread
generator transforms it to $I_{MT}$.




\PP{Multi-Thread Executor}
%
A 





\subsection{Implementation}
\label{ss:impl}

We implement \sys in various software layers as follows:
%
The target kernel instrumentation~(\autoref{ss:instrumentation}) is
implemented in two parts, a compiler pass and callback functions. The
compiler pass is implemented on the LLVM compiler suite
12.0.1~\cite{llvm} by modifying two existing LLVM passes, the
\texttt{SanitizerCoverage} pass~\cite{kcovpass} (for tracing basic
blocks) and the \texttt{ThreadSanitizer} pass~\cite{tsanpass} (for
tracing memory accesses).
%
And, the callback functions are implemented in the Linux kernel source
tree.
%
Thus, when compiling the kernel, our callbacks are also compiled into
the kernel binary.


The execution engine~(\autoref{ss:engine}) is implemented on QEMU
6.0.0, and leverages KVM (Kernel-based Virtual Machine) to take
advantage of hardware acceleration.
%
To allow hypercalls from the guest user space, we also modify the KVM
module of the host kernel.
%
In addition, the trampoline is enclosed into the target kernel source
tree as a loadable module.


Lastly, the \sys's userspace fuzzer~(\autoref{ss:fuzzer}) is
implemented based on \texttt{Syzkaller}~\cite{syzkaller}, a kernel
fuzzer developed by Google.



\PP{Lines of Code}
%
We use scc~\cite{scc} and sloccount~\cite{sloccount} to measure LoC of
GO and C, C++ respectively.
%
To summarize, the target kernel
instrumentation~(\autoref{ss:instrumentation}) is implemented with 323
LoC in C++ (for the compiler pass) and 265 LoC in C (for callback
functions).
%
The execution engine~(\autoref{ss:engine}) is implemented with 1662
LoC in C. And the userspace fuzzer~(\autoref{ss:fuzzer}) is
implemented with 3334 LoC in GoLang and 341 LoC in C++.

% the compiler pass is implmented in 323 lines of C++
% the callback functions are implmeneted in 265 lines of C
% the execution engine is implemented in 1662 lines of C
% the trampoline is 66 in C
% c2fuzz:
% go 3334
% C++ 341







%%% Local Variables:
%%% mode: latex
%%% TeX-master: "p"
%%% End:
